= Machine Learning Benchmarks

:toc: auto
:showtitle:

== System Tuning

The binding configuration for the GPUs needs to be correct.

The idea is to minimize latency going from CPU to GPU. Typically you have NUMA domains, you specify the closest NUMA domain for each CPU. 
In our system you only have half the PCI lanes, so you have to make a mapping based on cores.

See the current mapping using the following:

[,bash]
----
nvidia-smi topo -m
----

Example:

[,console]
----
root@o186i221:~/ccarlson/experiments# nvidia-smi topo -m
	GPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	GPU7	NIC0	NIC1	NIC2	NIC3	NIC4	CPU Affinity	NUMA Affinity
GPU0	 X 	PXB	SYS	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	SYS	48-63	3
GPU1	PXB	 X 	SYS	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	SYS	48-63	3
GPU2	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	16-31	1
GPU3	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	16-31	1
GPU4	SYS	SYS	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	PXB	SYS	SYS	112-127	7
GPU5	SYS	SYS	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	PXB	SYS	SYS	112-127	7
GPU6	SYS	SYS	SYS	SYS	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	PXB	80-95	5
GPU7	SYS	SYS	SYS	SYS	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	PXB	80-95	5
NIC0	PXB	PXB	SYS	SYS	SYS	SYS	SYS	SYS	 X 	SYS	SYS	SYS	SYS
NIC1	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	SYS	 X 	SYS	SYS	SYS
NIC2	SYS	SYS	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	 X 	SYS	SYS
NIC3	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	 X 	SYS
NIC4	SYS	SYS	SYS	SYS	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	 X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks

NIC Legend:

  NIC0: mlx5_0
  NIC1: mlx5_1
  NIC2: mlx5_2
  NIC3: mlx5_3
  NIC4: mlx5_4
----

In order to prioritize CUDA devices when building with Docker, set the `BUILDDOCKER_BUILDKIT` environment variable.

[,console]
----
BUILDDOCKER_BUILDKIT=1 docker build 
----

== MLPerf

* https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/[Nvidia - MLPerf]
* https://mlcommons.org/en/[MLCommons]

> MLPerfâ„¢ benchmarks - developed by MLCommons, a consortium of AI leaders from academia, research labs, and industry - are designed to provide unbiased evaluations of training and inference performance for hardware, software, and services. They're all conducted under prescribed conditions. To stay on the cutting edge of industry trends, MLPerf continues to evolve, holding new tests at regular intervals and adding new workloads that represent the state of the art in AI.

=== MLPerf Training

MLPerf Training presents three unique benchmarking challenges missing from other domains. Optimizations that improve training throughput can
increase time to solution, training is randomly determined and time to solution exhibits high variance,
and software and hardware are diverse enough to present difficulties for fairer benchmarking with the same binary, code, and even hyperparameters.
MLPerf Training tests eight different workloads across various use cases like computer vision, large language models, and recommenders.

While the repository, https://github.com/mlcommons/training[mlcommons/training], is intended to serve as valid starting points for benchmark implementations,
it is not meant to be used for real performance measurements of software frameworks and hardware.
The following steps would need to be executed in order to run a benchmark.
Speeds may vary based on the reference hardware being used.

1. Clone the repo https://github.com/mlcommons/training[mlcommons/training] to the host machine.
2. Run the `install_cuda_docker.sh` script on the host machine. This will ensure that CUDA and Docker is installed. The script would also take care of setting up the correct docker dependencies.
3. While being on the host machine and outside of docker, run `./download_dataset.sh` for the dataset being used for the benchmark. The script should be run within the directory of the benchmark.
4. Once the download completes, it can be verified by running `./verify_dataset.sh` in the same manner.
5. Build and run the docker image. Each benchmark has a command to do that.
6. Once the target quality is reached, the benchmark will stop to produce timing results.

=== MLPerf Storage

https://github.com/mlcommons/storage/blob/main/README.md[MLCommons - Storage]

MLPerf Storage is a benchmark suite to characterize the performance of storage systems that support machine learning workloads.

* https://github.com/mlcommons/storage/blob/main/README.md#overview[Overview]
* https://github.com/mlcommons/storage/blob/main/README.md#installation[Installation]
* https://github.com/mlcommons/storage/blob/main/README.md#configuration[Configuration]
* https://github.com/mlcommons/storage/blob/main/README.md#workloads[Workloads]
** https://github.com/mlcommons/storage/blob/main/README.md#u-net3d[U-Net3D]
** https://github.com/mlcommons/storage/blob/main/README.md#bert[BERT]
* https://github.com/mlcommons/storage/blob/main/README.md#parameters[Parameters]
** https://github.com/mlcommons/storage/blob/main/README.md#closed[CLOSED]
** https://github.com/mlcommons/storage/blob/main/README.md#open[OPEN]

The MLPerf Storage Benchmark Suite is an AI/ML benchmarking suite that measures performance of storage for ML workloads.
The benchmark helps bridge the gap between the utilization between storage and compute resources to make sure that they are both used efficiently for ML workloads.

It measures the sustained performance of a storage system for MLPerf Training and HPC workloads on both PyTorch and Tensorflow without requiring the use of expensive accelerators. Additionally, the `dlio_benchmark` code is used to emulate I/O patterns for deep learning workloads.

==== Accelerator Utilization

The Accelerator Utilization (AU) is the benchmark output metric samples per second for each workload.
In order the calculate the AU, the total compute time and the total benchmark running time is needed.
The total compute time is calculated by:

[,console]
----
total_compute_time = (records/file * total_files)/simulated_accelerators/batch_size * sleep_time
----

The formula below calculates the AU:

[,console]
----
AU (percentage) = (total_compute_time/total_benchmark_running_time) * 100 
----

==== MLPerf Storage Installation

First, the `mpich` for MPI package is needed. This can be done by:

[,bash]
----
sudo apt-get install mpich
----

Next, clone the https://github.com/mlcommons/storage[MLCommons Storage] repo.

[,bash]
----
git clone https://github.com/mlcommons/storage.git
----

The `requirements.txt` for `dlio_benchmark` would need to be installed. Before that, a Python virtual environment needs to be set up:

[,bash]
----
python3 -m venv ~/myvenv
source ~/myvenv/bin/activate
----

The `requirements.txt` would then be installed afterwards.

[,bash]
----
pip install -r dlio_benchmark/requirements.txt
----

To launch the `dlio_benchmark`, execute the `benchmark.sh` script:

[,bash]
----
./benchmark.sh -h
----

=== RESNET-50

* https://github.com/mlcommons/training/blob/master/object_detection/README.md[RESNET-50 Object Detection Instructions]
* https://github.com/mlcommons/training_results_v3.0/tree/main/NVIDIA/benchmarks/resnet/implementations/mxnet[Nvidia RESNET Instructions]

The RESNET-50 neural network is a well-known image classification network that can be used with the `ImageNet` dataset. It computationally intensive and good indication of driving meaningful storage I/O. In order to keep up the training benchmark and the overall run average time, the storage system has to keep up with the read bandwidth demands of a complete training job. The training benchmark are measured at `Epoch 0`, which is the most I/O-intensive portion of the MLPerf benchmark run. On that note, the RESNET-50 test would verify if the storage system is not a bottleneck for the workload and will provide the same images/second for `Epoch 0`.

==== RESNET-50 Installation

In order to run the MLPerf RESNET-50 test, https://github.com/mlcommons/ck/tree/master/cm/cmind[Collective Mind automation language] (CM) (also known as ML Commons CM language)  would need to be installed. It is part of the MLCommons Collective Knowledge (CK) project, and is powered by Python, JSON, YAML, and a unified CLI. More detailed information on CM  can be found here: https://github.com/mlcommons/ck/tree/master/cm#readme[Collective Minds]. The following installation steps assume the host machine will be running on RedHat Enterprise Linux. Furthermore, `python 3+`, `pip`, `git` , and `wget`  would need to be installed beforehand.

The following will install CM:

[,bash]
----
sudo dnf update
sudo dnf install python3 python-pip git wget curl
python3 -m pip install --user cmind
----

Once CM is installed, the next step would be to install MLCommons CK repository with automation workflows for MLPerf:

[,bash]
----
cm pull repo mlcommons@ck
----

This command will pre-process the dataset for a given backend. The `loadgen` would be built, and it will run the inference for all scenarios and modes.
A submission folder would be created with the test results.

[,bash]
----
cmr "run mlperf inference generate-run-cmds _submission" \
    --quiet --submitter="MLCommons" --hw_name=default --model=resnet50 --implementation=reference \
    --backend=onnxruntime --device=cpu --scenario=Offline --adr.compiler.tags=gcc  --target_qps=1 \
    --category=edge --division=open
----

The `target_qps` value would need to be updated according to the system performance for a valid submission.
The following values should be as is,

* Use `--device=cuda` to run the inference on Nvidia GPU
* Use `--division=closed` to run all scenarios for the closed division (compliance tests are skipped for `_find-performance` mode)
* Use `--category=datacenter` to run datacenter scenarios
* Use `--backend=tf` or `--backend=tvm-onnx` to use `tensorflow` and `tvm-onnx` backends, respectively


NOTE: Credit goes to https://www.linkedin.com/in/sakib-samar-23a79612a[Sakib Samar] for a large portion of these notes.

=== NCCL Tests

These tests check both the performance and the correctness of https://github.com/nvidia/nccl[NCCL] operations (inter-GPU communication).

GitHub repositories with documentation:

* https://github.com/NVIDIA/nccl-tests/tree/master[NCCL Tests]
* https://github.com/nvidia/nccl[NCCL]

==== Installation

First, you need `nccl` installed on the nodes you want to include in the benchmark.

Go ahead and clone the `nccl` repo:

[,bash]
----
git clone https://github.com/NVIDIA/nccl.git
----

Change directory into the `nccl` repo and build it against your system:

[,bash]
----
cd nccl
make -j src.build
----

Now, install it (here we're using Ubuntu 22.04):

[,bash]
----
# Install tools to create debian packages
sudo apt install build-essential devscripts debhelper fakeroot
# Build NCCL deb package
make pkg.debian.build
ls build/pkg/deb/
----

Your `.deb` packaages should now be in `build/pkg/deb`. You can install them using

[,bash]
----
apt install /root/ccarlson/nccl/build/pkg/deb/libnccl-dev_2.18.5-1+cuda12.2_amd64.deb /root/ccarlson/nccl/build/pkg/deb/libnccl2_2.18.5-1+cuda12.2_amd64.deb
----

Next, clone your `nccl-tests` repo:

[,bash]
----
git clone https://github.com/NVIDIA/nccl-tests.git
----

Now, change directory into it and `make` it against the installation of MPI you have on the system:

[,bash]
----
cd nccl-tests
make MPI=1 MPI_HOME=/usr/mpi/gcc/openmpi-4.1.5rc2
----

You should now have `nccl-tests` binaries available under `build/`:

[,console]
----
root@o186i221:~/ccarlson/nccl-tests# ls build/
all_gather_perf  alltoall_perf   gather_perf     reduce_perf          scatter_perf   timer.o
all_reduce_perf  broadcast_perf  hypercube_perf  reduce_scatter_perf  sendrecv_perf  verifiable
----

==== Usage

https://github.com/NVIDIA/nccl-tests/tree/master#arguments[NCCL Tests Arguments]

`all_reduce_perf` options:

[,console]
----
USAGE: all_reduce_perf
	[-t,--nthreads <num threads>]
	[-g,--ngpus <gpus per thread>]
	[-b,--minbytes <min size in bytes>]
	[-e,--maxbytes <max size in bytes>]
	[-i,--stepbytes <increment size>]
	[-f,--stepfactor <increment factor>]
	[-n,--iters <iteration count>]
	[-m,--agg_iters <aggregated iteration count>]
	[-w,--warmup_iters <warmup iteration count>]
	[-p,--parallel_init <0/1>]
	[-c,--check <check iteration count>]
	[-o,--op <sum/prod/min/max/avg/mulsum/all>]
	[-d,--datatype <nccltype/all>]
	[-r,--root <root>]
	[-z,--blocking <0/1>]
	[-y,--stream_null <0/1>]
	[-T,--timeout <time in seconds>]
	[-G,--cudagraph <num graph launches>]
	[-C,--report_cputime <0/1>]
	[-a,--average <0/1/2/3> report average iteration time <0=RANK0/1=AVG/2=MIN/3=MAX>]
	[-h,--help]
----

Running the NCCL tests on a single node is pretty straightforward:

[,bash]
----
/root/ccarlson/nccl-tests/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1
----

Running across multiple nodes should be done with MPI (`mpirun`)

[,bash]
----
mpirun --allow-run-as-root -np 2 --mca btl_tcp_if_include eth0 --machinefile machinefile.txt --map-by "node" /root/ccarlson/nccl-tests/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1
----

My `machinefile` for two nodes looks like:

[,console]
----
o186i221 slots=64
o186i222 slots=64
----

This launches two tasks via MPI (`-np 2`), one per node, each of which runs the `all_reduce_perf` binary with the specified options.

NOTE: We have to include the `eth0` interface for MPI to communicate over, otherwise it'll try to send TCP over the IB devices which won't work.

Example output for multiple nodes:

[,console]
----
/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1
# nThread 1 nGpus 8 minBytes 134217728 maxBytes 1073741824 step: 2(factor) warmup iters: 5 iters: 1 agg iters: 1 validation: 1 graph: 0
#
# Using devices
#  Rank  0 Group  0 Pid 256912 on   o186i221 device  0 [0x07] NVIDIA A100-SXM4-80GB
#  Rank  1 Group  0 Pid 256912 on   o186i221 device  1 [0x0b] NVIDIA A100-SXM4-80GB
#  Rank  2 Group  0 Pid 256912 on   o186i221 device  2 [0x48] NVIDIA A100-SXM4-80GB
#  Rank  3 Group  0 Pid 256912 on   o186i221 device  3 [0x4c] NVIDIA A100-SXM4-80GB
#  Rank  4 Group  0 Pid 256912 on   o186i221 device  4 [0x88] NVIDIA A100-SXM4-80GB
#  Rank  5 Group  0 Pid 256912 on   o186i221 device  5 [0x8b] NVIDIA A100-SXM4-80GB
#  Rank  6 Group  0 Pid 256912 on   o186i221 device  6 [0xc8] NVIDIA A100-SXM4-80GB
#  Rank  7 Group  0 Pid 256912 on   o186i221 device  7 [0xcb] NVIDIA A100-SXM4-80GB
#  Rank  8 Group  0 Pid 540236 on   o186i222 device  0 [0x07] NVIDIA A100-SXM4-80GB
#  Rank  9 Group  0 Pid 540236 on   o186i222 device  1 [0x0b] NVIDIA A100-SXM4-80GB
#  Rank 10 Group  0 Pid 540236 on   o186i222 device  2 [0x48] NVIDIA A100-SXM4-80GB
#  Rank 11 Group  0 Pid 540236 on   o186i222 device  3 [0x4c] NVIDIA A100-SXM4-80GB
#  Rank 12 Group  0 Pid 540236 on   o186i222 device  4 [0x88] NVIDIA A100-SXM4-80GB
#  Rank 13 Group  0 Pid 540236 on   o186i222 device  5 [0x8b] NVIDIA A100-SXM4-80GB
#  Rank 14 Group  0 Pid 540236 on   o186i222 device  6 [0xc8] NVIDIA A100-SXM4-80GB
#  Rank 15 Group  0 Pid 540236 on   o186i222 device  7 [0xcb] NVIDIA A100-SXM4-80GB
#
#                                                              out-of-place                       in-place
#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
   134217728      33554432     float     sum      -1   5619.6   23.88   44.78      0   5662.5   23.70   44.44      0
   268435456      67108864     float     sum      -1    10796   24.86   46.62      0    10836   24.77   46.45      0
   536870912     134217728     float     sum      -1    14128   38.00   71.25      0    13863   38.73   72.61      0
  1073741824     268435456     float     sum      -1    24419   43.97   82.45      0    24255   44.27   83.00      0
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 61.4514
#
----

