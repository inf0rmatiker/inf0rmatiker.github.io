= Installing Kubernetes

== OpenSUSE Leap 15.3

Original documentation for how to install `kubeadm`: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

. Open these ports on the firewall: https://kubernetes.io/docs/reference/ports-and-protocols/
a. `firewall-cmd --add-port=6443/tcp --permanent`
b. Repeat for all required ports
. Install `conntrack`: `zypper install conntrack-tools`
. Install `socat`: `zypper install socat`
. Enable `ip_forward`: `echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward`
 ** Follow this guide: https://kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic
. Install `containerd` as the container runtime: Follow instructions on https://github.com/containerd/containerd/blob/main/docs/getting-started.md[containerd's Getting Started page]
 ** Go get runc and CNI plugin files (see following steps)
 ** Go get the https://github.com/containerd/containerd/releases[latest released containerd archive]
 ** Untar it to `/usr/local`: `tar Cxzvf /usr/local containerd-1.6.6-linux-amd64.tar.gz`
 ** Download https://github.com/containerd/containerd/blob/main/containerd.service[containerd.service] and copy it to `/usr/lib/systemd/system/containerd.service` (Official docs say to put it at `/usr/local/lib`, there's nothing there and this won't work. Put it in `/usr/lib/…`)
 ** Launch systemd daemon for containerd:
  *** `systemctl daemon-reload`
  *** `systemctl enable --now containerd`
 ** Get runc archive if you don't already have it: https://github.com/opencontainers/runc/releases
 ** Install it: `install -m 755 runc.amd64 /usr/local/sbin/runc`
 ** Go get CNI plugins: https://github.com/containernetworking/plugins/releases, and install them under `opt/cni/bin`:
  *** `mkdir -p /opt/cni/bin`
  *** `tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz`
 ** Check it works by running `ctr --help`
 ** Generate default containerd daemon configuration: `containerd config default > /etc/containerd/config.toml`
. Configure systemd as cgroup with runc:
 ** Edit `/etc/containerd/config.toml`:
+
----
   [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
       ...
       [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
       SystemdCgroup = true
----

 ** Restart containerd: `sudo systemctl restart containerd`
. Install `kubeadm`: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
. Install `critcl`:
+
[,bash]
----
 DOWNLOAD_DIR=/usr/local/bin
 sudo mkdir -p $DOWNLOAD_DIR
 VERSION="v1.24.2"
 wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz
 sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin
 rm -f crictl-$VERSION-linux-amd64.tar.gz
----

. Install `kubeadm`, `kubectl`, `kubelet`
+
[,bash]
----
 RELEASE="$(curl -sSL https://dl.k8s.io/release/stable.txt)"`
 ARCH="amd64"
 cd $DOWNLOAD_DIR
 sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet,kubectl}
 sudo chmod +x {kubeadm,kubelet,kubectl}
 RELEASE_VERSION="v0.4.0"
 curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service
 sudo mkdir -p /etc/systemd/system/kubelet.service.d
 curl -sSL "https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:${DOWNLOAD_DIR}:g" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
----

. Enable kubelet: `systemctl enable --now kubelet`

[,bash]
----
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

 export KUBECONFIG=/etc/kubernetes/admin.conf
----

. You should now deploy a pod network to the cluster.
 ** Run `kubectl apply -f <podnetwork>.yaml` with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/
. Then you can join any number of worker nodes by running the following on each as `root`:

[,bash]
----
kubeadm join 172.26.143.70:6443 --token oos4yr.46zwgo9mzqavckkx --discovery-token-ca-cert-hash sha256:2b725a2cda814b07ee07c9d704de5a5cc2451c746eeb5b32277ebe661b9a36e4
----

== Rocky Linux 9.5

This example install will be on the `mawenzi` cluster, using `mawenzi-01` as the admin node.

=== References

* https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab0-README/[Rocky: Kubernetes the Hard Way]
* https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/[Kubernetes: Bootstrapping a cluster with kubeadm]
** https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/[Installing kubeadm]
* https://kubernetes.io/docs/concepts/overview/components/[Kubernetes: Components]

=== Prerequisites

We'll start by preparing things for the control-plane node.

*Proxies*:

Set up DNF HPE proxy so we can download things with DNF from the internet.

[,bash]
----
cat >> /etc/dnf/dnf.conf << EOF
[main]
gpgcheck=0
installonly_limit=3
clean_requirements_on_remove=True
best=True
skip_if_unavailable=False
proxy=http://proxy.houston.hpecorp.net:8080
EOF
----

Set up HTTP HPE proxy so we can download things with `wget`, etc, from the internet.
We'll need to disable this later on when setting up kubernetes; otherwise internal
requests will try to use the proxy and will fail.

[,bash]
----
cat >> /etc/environment << EOF
http_proxy="http://proxy.houston.hpecorp.net:8080/"
https_proxy="http://proxy.houston.hpecorp.net:8080/"
ftp_proxy="http://proxy.houston.hpecorp.net:8080/"
no_proxy="admin,localhost,127.0.0.1,.us.cray.com,.hpe.com"
EOF
----

*Install utilities*:

Upgrade as many packages, and the kernel with DNF as we can before we get started:

[,bash]
----
dnf upgrade
----

Download basic CLI utilities

[,bash]
----
dnf -y install wget curl vim openssl git tar conntrack-tools socat
----

=== Enable IPv4 forwarding

[,bash]
----
modprobe bridge
modprobe br_netfilter
echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.conf
sysctl -p /etc/sysctl.conf
echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward
----

=== Disable firewalld

[,bash]
----
systemctl stop firewalld
systemctl disable firewalld
----

=== Disable swap

[,bash]
----
# This only disables swap for the current session.
swapoff -a

# This will find any lines in /etc/fstab with 'swap' and comment them out
sed -i.bak '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
----

=== Disable SELinux

[,bash]
----
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
----

=== Install containerd, CNI plugin, and runc

https://github.com/containerd/containerd/blob/main/docs/getting-started.md

This step will also install `runc.amd64` and CNI plugins.

.download.sh
[,bash]
----
#!/bin/bash

# https://kubernetes.io/releases/
# KUBERNETES_VERSION="v1.33.0"

# https://github.com/opencontainers/runc/releases
RUNC_VERSION="v1.3.0"

# https://github.com/kubernetes-sigs/cri-tools/releases
CRI_TOOLS_VERSION="v1.33.0"

# https://github.com/containernetworking/plugins/releases
CNI_PLUGINS_VERSION="v1.7.1"

# https://github.com/containerd/containerd/releases
CONTAINERD_VERSION="2.1.4"

# https://github.com/etcd-io/etcd/releases/
# ETCD_VERSION="v3.6.4"

download_urls=(
	# https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kubectl
	# https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kube-apiserver
	# https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kube-controller-manager
	# https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kube-scheduler
	# https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kube-proxy
	# https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kubelet
	https://github.com/kubernetes-sigs/cri-tools/releases/download/$CRI_TOOLS_VERSION/crictl-$CRI_TOOLS_VERSION-linux-amd64.tar.gz
	https://github.com/opencontainers/runc/releases/download/$RUNC_VERSION/runc.amd64
	https://github.com/containernetworking/plugins/releases/download/$CNI_PLUGINS_VERSION/cni-plugins-linux-amd64-$CNI_PLUGINS_VERSION.tgz
	https://github.com/containerd/containerd/releases/download/v$CONTAINERD_VERSION/containerd-$CONTAINERD_VERSION-linux-amd64.tar.gz
	# https://github.com/etcd-io/etcd/releases/download/$ETCD_VERSION/etcd-$ETCD_VERSION-linux-amd64.tar.gz
)

for download_url in "${download_urls[@]}"; do
	wget -q --show-progress  --https-only --timestamping "$download_url"
done
----

Extract containerd tarball to `/usr/local`:

[,bash]
----
tar Cxzvf /usr/local containerd-*-linux-amd64.tar.gz
----

We'll be running containerd with systemd. This unit file can be found here, and
the below example uses modifications to the environment variables for the service
to use the HPE proxy. This will allow the containerd runtime to pull images from
outside the lab.

Generate a default containerd configuration file:

[,bash]
----
mkdir /etc/containerd
containerd config default > /etc/containerd/config.toml
----

Edit this file and add systemd as the cgroup driver for containerd.
https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd-systemd

[,plaintext]
----
[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runc]
  ...
  [plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.runc.options]
    SystemdCgroup = true
----

Install `runc.amd64`. This should have been downloaded from the `download.sh` script before, but if not,
you can get it from https://github.com/opencontainers/runc/releases.

[,bash]
----
install -m 755 runc.amd64 /usr/local/sbin/runc
----

Install CNI plugins. This should have been downloaded from the `download.sh` script before, but if not,
you can get it from https://github.com/containernetworking/plugins/releases.

[,bash]
----
mkdir -p /opt/cni/bin
tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v*.tgz
----

./usr/lib/systemd/system/containerd.service
[,plaintext]
----
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target dbus.service

[Service]
Environment="HTTP_PROXY=http://proxy.houston.hpecorp.net:8080/"
Environment="HTTPS_PROXY=http://proxy.houston.hpecorp.net:8080/"
Environment="FTP_PROXY=http://proxy.houston.hpecorp.net:8080/"
Environment="NO_PROXY=admin,localhost,127.0.0.1,.us.cray.com,.hpe.com,hpc.amslabs.hpecorp.net"
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
----

With this in place, enable and load containerd with systemd:

[,bash]
----
systemctl daemon-reload
systemctl enable --now containerd
----

Check the status. It should look like this:

[,console]
----
[root@mawenzi-01 downloads]# systemctl status containerd
● containerd.service - containerd container runtime
     Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; preset: disabled)
     Active: active (running) since Tue 2025-08-26 10:31:12 MDT; 3s ago
       Docs: https://containerd.io
    Process: 117075 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
   Main PID: 117077 (containerd)
      Tasks: 17
     Memory: 36.2M
        CPU: 194ms
     CGroup: /system.slice/containerd.service
             └─117077 /usr/local/bin/containerd

Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.426293642-06:00" level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc
Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.426334788-06:00" level=info msg="Start cni network conf syncer for default"
Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.426447839-06:00" level=info msg=serving... address=/run/containerd/containerd.sock
Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.426473006-06:00" level=info msg="Start streaming server"
Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.426507981-06:00" level=info msg="Registered namespace \"k8s.io\" with NRI"
Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.426545321-06:00" level=info msg="runtime interface starting up..."
Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.426564517-06:00" level=info msg="starting plugins..."
Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.426601917-06:00" level=info msg="Synchronizing NRI (plugin) with current runtime state"
Aug 26 10:31:12 mawenzi-01 containerd[117077]: time="2025-08-26T10:31:12.427695697-06:00" level=info msg="containerd successfully booted in 0.121902s"
Aug 26 10:31:12 mawenzi-01 systemd[1]: Started containerd container runtime.
----

=== Install Kubernetes packages

This gets us `kubeadm`, `kubectl`, and `kubelet`.

[,bash]
----
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF

dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
----

=== Enable kubelet systemd service

Enable the kubelet service.

[,bash]
----
systemctl enable --now kubelet
----

Note: If you check the status of the kubelet service (`systemctl status kubelet`), you'll see that it's
exiting with code FAILURE. This is normal. It's restarting every few seconds, while we wait for `kubeadm`
to tell it what to do.


=== Using kubeadm to create cluster

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

We'll take a break from setting up the control-plane node, and go ahead and start
configuring the other worker nodes in the cluster.








=== Download binaries

https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab2-jumpbox/#download-binaries

Make a Downloads directory

[,console]
----
[root@mawenzi-01 ~]# mkdir downloads
[root@mawenzi-01 ~]# cd downloads/
----

Create a script to help download all the binary components needed for the cluster.
Adjust the versions based on the most recent released versions of things.
These are the versions we're using for this example.

.download.sh
[,bash]
----
#!/bin/bash

# https://kubernetes.io/releases/
KUBERNETES_VERSION="v1.33.0"

# https://github.com/opencontainers/runc/releases
RUNC_VERSION="v1.3.0"

# https://github.com/kubernetes-sigs/cri-tools/releases
CRI_TOOLS_VERSION="v1.33.0"

# https://github.com/containernetworking/plugins/releases
CNI_PLUGINS_VERSION="v1.7.1"

# https://github.com/containerd/containerd/releases
CONTAINERD_VERSION="2.1.4"

# https://github.com/etcd-io/etcd/releases/
ETCD_VERSION="v3.6.4"

download_urls=(
	https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kubectl
	https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kube-apiserver
	https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kube-controller-manager
	https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kube-scheduler
	https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kube-proxy
	https://dl.k8s.io/$KUBERNETES_VERSION/bin/linux/amd64/kubelet
	https://github.com/kubernetes-sigs/cri-tools/releases/download/$CRI_TOOLS_VERSION/crictl-$CRI_TOOLS_VERSION-linux-amd64.tar.gz
	https://github.com/opencontainers/runc/releases/download/$RUNC_VERSION/runc.amd64
	https://github.com/containernetworking/plugins/releases/download/$CNI_PLUGINS_VERSION/cni-plugins-linux-amd64-$CNI_PLUGINS_VERSION.tgz
	https://github.com/containerd/containerd/releases/download/v$CONTAINERD_VERSION/containerd-$CONTAINERD_VERSION-linux-amd64.tar.gz
	https://github.com/etcd-io/etcd/releases/download/$ETCD_VERSION/etcd-$ETCD_VERSION-linux-amd64.tar.gz
)

for download_url in "${download_urls[@]}"; do
	wget -q --show-progress  --https-only --timestamping "$download_url"
done

----

Execute the script (after `chmod +x download.sh`):

[,console]
----
[root@mawenzi-01 Downloads]# ./download.sh
kubectl                                    100%[=======================================================================================>]  57.34M  29.8MB/s    in 1.9s
kube-apiserver                             100%[=======================================================================================>]  93.42M  23.2MB/s    in 4.1s
kube-controller-manager                    100%[=======================================================================================>]  86.55M  21.2MB/s    in 4.2s
kube-scheduler                             100%[=======================================================================================>]  66.38M  23.5MB/s    in 2.8s
kube-proxy                                 100%[=======================================================================================>]  67.32M  20.2MB/s    in 3.3s
kubelet                                    100%[=======================================================================================>]  77.91M  28.5MB/s    in 2.7s
crictl-v1.33.0-linux-amd64.tar.gz          100%[=======================================================================================>]  19.43M  16.0MB/s    in 1.2s
runc.amd64                                 100%[=======================================================================================>]  11.31M  13.0MB/s    in 0.9s
cni-plugins-linux-amd64-v1.7.1.tgz         100%[=======================================================================================>]  53.31M  17.2MB/s    in 3.1s
containerd-2.1.4-linux-amd64.tar.gz        100%[=======================================================================================>]  31.67M  14.0MB/s    in 2.3s
etcd-v3.6.4-linux-amd64.tar.gz             100%[=======================================================================================>]  22.53M  13.4MB/s    in 1.7s
----

Make the binaries executable: `chmod +x kube* runc.amd64`.

=== Install kubectl

https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab2-jumpbox/#install-kubectl

[,bash]
----
cp kubectl /usr/local/bin/
----

Verify version:

[,console]
----
[root@mawenzi-01 Downloads]# kubectl version --client
Client Version: v1.33.0
Kustomize Version: v5.6.0
----

=== Set up machines file

https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab3-compute-resources/#machine-database

Create a `machines.txt` file with the format: `IPV4_ADDRESS FQDN HOSTNAME POD_SUBNET`.

Each column corresponds to a machine IP address `IPV4_ADDRESS`, fully qualified domain name `FQDN`,
host name `HOSTNAME`, and the IP subnet `POD_SUBNET`. Kubernetes assigns one IP address per pod,
and the `POD_SUBNET` represents the unique IP address range assigned to each machine in the cluster for doing so.

.machines.txt
[,console]
----
10.214.134.147 mawenzi-01.hpc.amslabs.hpecorp.net mawenzi-01
10.214.130.159 mawenzi-02.hpc.amslabs.hpecorp.net mawenzi-02 10.200.0.0/24
10.214.134.195 mawenzi-03.hpc.amslabs.hpecorp.net mawenzi-03 10.200.0.1/24
----

=== Set up worker nodes SSH access

With a blank install, you won't have any SSH keys generated:

[,console]
----
[root@mawenzi-01 kubernetes]# ls -la ~/.ssh
total 16
drwx------. 2 root root   71 Aug 22 11:57 .
dr-xr-x---. 5 root root 4096 Aug 22 11:53 ..
-rw-------. 1 root root  195 Aug 22 10:02 authorized_keys
-rw-------. 1 root root  828 Aug 22 11:57 known_hosts
-rw-r--r--. 1 root root   92 Aug 22 11:57 known_hosts.old
----

Use `ssh-keygen` to generate a public/private key pair.

Copy the public key to each node in the `machines.txt` file.

[,bash]
-----
while read IP FQDN HOST SUBNET; do
  ssh-copy-id root@${IP}
done < machines.txt
-----

[,console]
----
[root@mawenzi-01 kubernetes]# ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa
Your public key has been saved in /root/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:QxH4vc8bmPmzyAKa+HPk0nUsnRPdXIf51fBJx98zO0c root@mawenzi-01
The key's randomart image is:
+---[RSA 3072]----+
|       .o.    .*o|
|      .  .    +.O|
|       ... . o +*|
|       .. o . ooE|
|        So +   .+|
|      o o.B+   o.|
|   . * o o++.   o|
|  . = + .. o+.   |
|   ..+   .o ++   |
+----[SHA256]-----+
----

=== Set up hostnames

https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab3-compute-resources/#hostnames

N/A: We already have hostnames from the lab so maybe we don't need to do this?

=== Provision a CA and TLS certificates

https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab4-certificate-authority/

In this lab, you will provision a PKI Infrastructure using OpenSSL to bootstrap a Certificate Authority and generate TLS certificates for the following components:

* kube-apiserver
* kube-controller-manager
* kube-scheduler
* kubelet
* kube-proxy

In this section, you will provision a Certificate Authority that you will use to generate additional
TLS certificates for the other Kubernetes components. Setting up CA and generating certificates with
openssl can be time-consuming, especially when doing it for the first time.
To streamline this lab, an openssl configuration file, `ca.conf`, must be included,
which defines all the details needed to generate certificates for each Kubernetes component.

We'll use the following `ca.conf` Certificate Authority configuration file:

.ca.conf
[,text]
----
[req]
distinguished_name = req_distinguished_name
prompt             = no
x509_extensions    = ca_x509_extensions

[ca_x509_extensions]
basicConstraints = CA:TRUE
keyUsage         = cRLSign, keyCertSign

[req_distinguished_name]
C   = US
ST  = Colorado
L   = Denver
CN  = CA

[admin]
distinguished_name = admin_distinguished_name
prompt             = no
req_extensions     = default_req_extensions

[admin_distinguished_name]
CN = admin
O  = system:masters

# Service Accounts
#
# The Kubernetes Controller Manager leverages a key pair to generate
# and sign service account tokens as described in the
# [managing service accounts](https://kubernetes.io/docs/admin/service-accounts-admin/)
# documentation.

[service-accounts]
distinguished_name = service-accounts_distinguished_name
prompt             = no
req_extensions     = default_req_extensions

[service-accounts_distinguished_name]
CN = service-accounts

# Worker Nodes
#
# Kubernetes uses a [special-purpose authorization mode](https://kubernetes.io/docs/admin/authorization/node/)
# called Node Authorizer, that specifically authorizes API requests made
# by [Kubelets](https://kubernetes.io/docs/concepts/overview/components/#kubelet).
# In order to be authorized by the Node Authorizer, Kubelets must use a credential
# that identifies them as being in the `system:nodes` group, with a username
# of `system:node:<nodeName>`.
[mawenzi-02]
distinguished_name = mawenzi-02_distinguished_name
prompt             = no
req_extensions     = mawenzi-02_req_extensions

[mawenzi-02_req_extensions]
basicConstraints     = CA:FALSE
extendedKeyUsage     = clientAuth, serverAuth
keyUsage             = critical, digitalSignature, keyEncipherment
nsCertType           = client
nsComment            = "mawenzi-02 Certificate"
subjectAltName       = DNS:mawenzi-02, IP:127.0.0.1
subjectKeyIdentifier = hash

[mawenzi-02_distinguished_name]
CN = system:node:mawenzi-02
O  = system:nodes
C  = US
ST = Colorado
L  = Denver

[mawenzi-03]
distinguished_name = mawenzi-03_distinguished_name
prompt             = no
req_extensions     = mawenzi-03_req_extensions

[mawenzi-03_req_extensions]
basicConstraints     = CA:FALSE
extendedKeyUsage     = clientAuth, serverAuth
keyUsage             = critical, digitalSignature, keyEncipherment
nsCertType           = client
nsComment            = "mawenzi-03 Certificate"
subjectAltName       = DNS:mawenzi-03, IP:127.0.0.1
subjectKeyIdentifier = hash

[mawenzi-03_distinguished_name]
CN = system:node:mawenzi-03
O  = system:nodes
C  = US
ST = Colorado
L  = Denver


# Kube Proxy Section
[kube-proxy]
distinguished_name = kube-proxy_distinguished_name
prompt             = no
req_extensions     = kube-proxy_req_extensions

[kube-proxy_req_extensions]
basicConstraints     = CA:FALSE
extendedKeyUsage     = clientAuth, serverAuth
keyUsage             = critical, digitalSignature, keyEncipherment
nsCertType           = client
nsComment            = "Kube Proxy Certificate"
subjectAltName       = DNS:kube-proxy, IP:127.0.0.1
subjectKeyIdentifier = hash

[kube-proxy_distinguished_name]
CN = system:kube-proxy
O  = system:node-proxier
C  = US
ST = Colorado
L  = Denver


# Controller Manager
[kube-controller-manager]
distinguished_name = kube-controller-manager_distinguished_name
prompt             = no
req_extensions     = kube-controller-manager_req_extensions

[kube-controller-manager_req_extensions]
basicConstraints     = CA:FALSE
extendedKeyUsage     = clientAuth, serverAuth
keyUsage             = critical, digitalSignature, keyEncipherment
nsCertType           = client
nsComment            = "Kube Controller Manager Certificate"
subjectAltName       = DNS:kube-proxy, IP:127.0.0.1
subjectKeyIdentifier = hash

[kube-controller-manager_distinguished_name]
CN = system:kube-controller-manager
O  = system:kube-controller-manager
C  = US
ST = Colorado
L  = Denver

# Scheduler
[kube-scheduler]
distinguished_name = kube-scheduler_distinguished_name
prompt             = no
req_extensions     = kube-scheduler_req_extensions

[kube-scheduler_req_extensions]
basicConstraints     = CA:FALSE
extendedKeyUsage     = clientAuth, serverAuth
keyUsage             = critical, digitalSignature, keyEncipherment
nsCertType           = client
nsComment            = "Kube Scheduler Certificate"
subjectAltName       = DNS:kube-scheduler, IP:127.0.0.1
subjectKeyIdentifier = hash

[kube-scheduler_distinguished_name]
CN = system:kube-scheduler
O  = system:system:kube-scheduler
C  = US
ST = Colorado
L  = Denver


# API Server
#
# The Kubernetes API server is automatically assigned the `kubernetes`
# internal dns name, which will be linked to the first IP address (`10.32.0.1`)
# from the address range (`10.32.0.0/24`) reserved for internal cluster
# services.

[kube-api-server]
distinguished_name = kube-api-server_distinguished_name
prompt             = no
req_extensions     = kube-api-server_req_extensions

[kube-api-server_req_extensions]
basicConstraints     = CA:FALSE
extendedKeyUsage     = clientAuth, serverAuth
keyUsage             = critical, digitalSignature, keyEncipherment
nsCertType           = client
nsComment            = "Kube Scheduler Certificate"
subjectAltName       = @kube-api-server_alt_names
subjectKeyIdentifier = hash

[kube-api-server_alt_names]
IP.0  = 127.0.0.1
IP.1  = 10.32.0.1
DNS.0 = kubernetes
DNS.1 = kubernetes.default
DNS.2 = kubernetes.default.svc
DNS.3 = kubernetes.default.svc.cluster
DNS.4 = kubernetes.svc.cluster.local
DNS.5 = server.kubernetes.local
DNS.6 = api-server.kubernetes.local

[kube-api-server_distinguished_name]
CN = kubernetes
C  = US
ST = Colorado
L  = Denver


[default_req_extensions]
basicConstraints     = CA:FALSE
extendedKeyUsage     = clientAuth
keyUsage             = critical, digitalSignature, keyEncipherment
nsCertType           = client
nsComment            = "Admin Client Certificate"
subjectKeyIdentifier = hash
----

Every certificate authority starts with a private key and root certificate. In this section,
you will create a self-signed certificate authority, and while that is all you need for this tutorial,
this is something you should not consider in a real-world production environment.

Generate the CA configuration file, certificate, and private key:

[,bash]
----
openssl genrsa -out ca.key 4096
openssl req -x509 -new -sha512 -noenc \
    -key ca.key -days 3653 \
    -config ca.conf \
    -out ca.crt
----

You should now have `ca.crt` and `ca.key` in your directory.
You can view details of the `.crt` key by running: `openssl x509 -in ca.crt -text -noout | less`.

=== Create client/server certificates

In this section, you will generate client and server certificates for each Kubernetes
component and a client certificate for the Kubernetes admin user.

Generate the certificates and private keys:

.gen_keys.sh
[,bash]
----
#!/bin/bash

certs=(
  "admin" "mawenzi-02" "mawenzi-03"
  "kube-proxy" "kube-scheduler"
  "kube-controller-manager"
  "kube-api-server"
  "service-accounts"
)

for i in ${certs[*]}; do
  openssl genrsa -out "${i}.key" 4096

  openssl req -new -key "${i}.key" -sha256 \
    -config "ca.conf" -section ${i} \
    -out "${i}.csr"

  openssl x509 -req -days 3653 -in "${i}.csr" \
    -copy_extensions copyall \
    -sha256 -CA "ca.crt" \
    -CAkey "ca.key" \
    -CAcreateserial \
    -out "${i}.crt"
done
----

.Example
[,console]
----
[root@mawenzi-01 kubernetes]# ./gen_keys.sh
Certificate request self-signature ok
subject=CN=admin, O=system:masters
Certificate request self-signature ok
subject=CN=system:node:mawenzi-02, O=system:nodes, C=US, ST=Colorado, L=Denver
Certificate request self-signature ok
subject=CN=system:node:mawenzi-03, O=system:nodes, C=US, ST=Colorado, L=Denver
Certificate request self-signature ok
subject=CN=system:kube-proxy, O=system:node-proxier, C=US, ST=Colorado, L=Denver
Certificate request self-signature ok
subject=CN=system:kube-scheduler, O=system:system:kube-scheduler, C=US, ST=Colorado, L=Denver
Certificate request self-signature ok
subject=CN=system:kube-controller-manager, O=system:kube-controller-manager, C=US, ST=Colorado, L=Denver
Certificate request self-signature ok
subject=CN=kubernetes, C=US, ST=Colorado, L=Denver
Certificate request self-signature ok
subject=CN=service-accounts
----

The above command results will generate a private key, certificate request, and signed SSL certificate for each Kubernetes component.
You can list the generated files with the following command: `ls -1 *.crt *.key *.csr`

.Example
[,console]
----
admin.crt
admin.csr
admin.key
ca.crt
ca.key
kube-api-server.crt
kube-api-server.csr
kube-api-server.key
kube-controller-manager.crt
kube-controller-manager.csr
kube-controller-manager.key
kube-proxy.crt
kube-proxy.csr
kube-proxy.key
kube-scheduler.crt
kube-scheduler.csr
kube-scheduler.key
mawenzi-02.crt
mawenzi-02.csr
mawenzi-02.key
mawenzi-03.crt
mawenzi-03.csr
mawenzi-03.key
service-accounts.crt
service-accounts.csr
service-accounts.key
----

=== Distribute the keys

Use the following script to distribute the keys and certificates to the worker nodes:

[,bash]
----
#!/bin/bash

worker_nodes=("mawenzi-02" "mawenzi-03")

for host in "${worker_nodes[@]}"; do
  ssh root@$host mkdir /var/lib/kubelet/
  scp ca.crt root@$host:/var/lib/kubelet/

  scp $host.crt \
    root@$host:/var/lib/kubelet/kubelet.crt

  scp $host.key \
    root@$host:/var/lib/kubelet/kubelet.key
done
----

=== Generate Kubernetes config files for authentication

https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab5-kubernetes-configuration-files/#the-kubelet-kubernetes-configuration-file

Now we'll generate kubeconfig files for the `kubelet` and the `admin` user.

When generating kubeconfig files for Kubelets you must match the client certificate to the Kubelet's node name.
This will ensure Kubelets are properly authorized by the https://kubernetes.io/docs/reference/access-authn-authz/node/[Kubernetes Note Authorizer].

We'll use the following script, `gen_kubeconfigs.sh`, to generate the kubeconfig files for the node kubelets,
kube-proxy, kube-controller-manager, kube-scheduler, and admin node.

.gen_kubeconfigs.sh
[,bash]
----
#!/bin/bash

cluster_name="mawenzi"
worker_nodes=("mawenzi-02" "mawenzi-03")
master_node="mawenzi-01"
services=("kube-proxy" "kube-controller-manager" "kube-scheduler")

# Worker node kubelet kubeconfig files
for host in "${worker_nodes[@]}"; do
  echo "Generating kubeconfig files for worker '$host' kubelet"
  kubectl config set-cluster $cluster_name \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://$master_node:6443 \
    --kubeconfig=${host}.kubeconfig

  kubectl config set-credentials system:node:${host} \
    --client-certificate=${host}.crt \
    --client-key=${host}.key \
    --embed-certs=true \
    --kubeconfig=${host}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:${host} \
    --kubeconfig=${host}.kubeconfig

  kubectl config use-context default \
    --kubeconfig=${host}.kubeconfig
done

# kube-proxy, kube-controller-manager, kube-scheduler
for service in "${services[@]}"; do
  kubectl config set-cluster $cluster_name \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://$master:6443 \
    --kubeconfig=$service.kubeconfig

  kubectl config set-credentials system:$service \
    --client-certificate=$service.crt \
    --client-key=$service.key \
    --embed-certs=true \
    --kubeconfig=$service.kubeconfig

  kubectl config set-context default \
    --cluster=$cluster_name \
    --user=system:$service \
    --kubeconfig=$service.kubeconfig

  kubectl config use-context default \
    --kubeconfig=$service.kubeconfig
done

# admin
kubectl config set-cluster $cluster_name \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
    --client-certificate=admin.crt \
    --client-key=admin.key \
    --embed-certs=true \
    --kubeconfig=admin.kubeconfig

kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=admin \
    --kubeconfig=admin.kubeconfig

kubectl config use-context default \
    --kubeconfig=admin.kubeconfig

----

Running this:

[,console]
----
[root@mawenzi-01 kubernetes]# ./gen_kubeconfigs.sh
Generating kubeconfig files for worker 'mawenzi-02' kubelet
Cluster "mawenzi" set.
User "system:node:mawenzi-02" set.
Context "default" modified.
Switched to context "default".
Generating kubeconfig files for worker 'mawenzi-03' kubelet
Cluster "mawenzi" set.
User "system:node:mawenzi-03" set.
Context "default" modified.
Switched to context "default".
Cluster "mawenzi" set.
User "system:kube-proxy" set.
Context "default" modified.
Switched to context "default".
Cluster "mawenzi" set.
User "system:kube-controller-manager" set.
Context "default" created.
Switched to context "default".
Cluster "mawenzi" set.
User "system:kube-scheduler" set.
Context "default" created.
Switched to context "default".
Cluster "mawenzi" set.
User "admin" set.
Context "default" created.
Switched to context "default".
----

And, we get:

* `admin.kubeconfig`
* `kube-controller-manager.kubeconfig`
* `kube-proxy.kubeconfig`
* `kube-scheduler.kubeconfig`
* `mawenzi-02.kubeconfig`
* `mawenzi-03.kubeconfig`

=== Generate encryption configuration and key

https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab6-data-encryption-keys/#lab-6-generating-the-data-encryption-configuration-and-key

We'll generate an encryption key and an encryption configuration suitable for encrypting
 https://kubernetes.io/docs/concepts/configuration/secret/[Kubernetes Secrets].

Generate a random encryption key, using `/dev/urandom` and `base64`:

[,bash]
----
export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
----

.Example:
[,console]
----
[root@mawenzi-01 kubernetes]# echo $ENCRYPTION_KEY
wMCVvvm23ffr.............wx7naVcfsutjvED4Dg=
----

Create the following encryption config YAML file:

.encryption-config-template.yaml
[,yaml]
----
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
----

Then, substitute in the `$ENCRYPTION_KEY` variable:

[,bash]
----
envsubst < encryption-config-template.yaml > encryption-config.yaml
----

(it should look like this now)

.encryption-config.yaml
[,yaml]
----
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: wMCVvvm23ffr.............wx7naVcfsutjvED4Dg=
      - identity: {}
----

=== Bootstrapping the etcd cluster

https://docs.rockylinux.org/labs/kubernetes-the-hard-way/lab7-bootstrapping-etcd/#lab-7-bootstrapping-the-etcd-cluster

Create the following `etcd.service` systemd service file:

[,console]
----
[Unit]
Description=etcd
Documentation=https://github.com/etcd-io/etcd

[Service]
Type=notify
Environment="ETCD_UNSUPPORTED_ARCH=arm64"
ExecStart=/usr/local/bin/etcd \
  --name controller \
  --initial-advertise-peer-urls http://127.0.0.1:2380 \
  --listen-peer-urls http://127.0.0.1:2380 \
  --listen-client-urls http://127.0.0.1:2379 \
  --advertise-client-urls http://127.0.0.1:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller=http://127.0.0.1:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
----

[,bash]
----
mv etcd.service /etc/systemd/system/
chmod 644 /etc/systemd/system/etcd.service
----

=== Install etcd

Untar the etcd .tgz in `downloads/` and move its binaries to `/usr/local/bin`

[,bash]
----
tar -xzvf etcd-v3.6.4-linux-amd64.tar.gz
mv etcd-v3.6.4-linux-amd64/etcd* /usr/local/bin/
----

Back in the `kubernetes/` directory, configure the `etcd` server:

[,bash]
----
mkdir -p /etc/etcd /var/lib/etcd && \
  chmod 700 /var/lib/etcd && \
  cp ca.crt kube-api-server.key kube-api-server.crt /etc/etcd/
----

=== Disable SELinux/firewall

Although it is considered a bad security form, you might have to temporarily or permanently
disable SELinux if you run into any issues starting the etcd systemd service.
The proper fix is to investigate and create the needed policy files with tools
such as `ausearch`, `audit2allow`, and others.

The commands get SELinux out of the way and disable it by running the following:

[,bash]
----
sudo sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
setenforce 0
----

=== Start the etcd server

[,bash]
----
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
----

Check the status of etcd:

[,bash]
----
systemctl status etcd
etcdctl member list
----

=== Bootstrap the Kubernetes control plane

We'll need to create the following systemd service YAML files on the controller.

.kube-apiserver.service
[,plaintext]
----
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
  --allow-privileged=true \
  --apiserver-count=1 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/log/audit.log \
  --authorization-mode=Node,RBAC \
  --bind-address=0.0.0.0 \
  --client-ca-file=/var/lib/kubernetes/ca.crt \
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --etcd-servers=http://127.0.0.1:2379 \
  --event-ttl=1h \
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.crt \
  --kubelet-client-certificate=/var/lib/kubernetes/kube-api-server.crt \
  --kubelet-client-key=/var/lib/kubernetes/kube-api-server.key \
  --runtime-config='api/all=true' \
  --service-account-key-file=/var/lib/kubernetes/service-accounts.crt \
  --service-account-signing-key-file=/var/lib/kubernetes/service-accounts.key \
  --service-account-issuer=https://server.kubernetes.local:6443 \
  --service-cluster-ip-range=10.32.0.0/24 \
  --service-node-port-range=30000-32767 \
  --tls-cert-file=/var/lib/kubernetes/kube-api-server.crt \
  --tls-private-key-file=/var/lib/kubernetes/kube-api-server.key \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
----

.kube-controller-manager.service
[,plaintext]
----
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
  --bind-address=0.0.0.0 \
  --cluster-cidr=10.200.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.crt \
  --cluster-signing-key-file=/var/lib/kubernetes/ca.key \
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \
  --root-ca-file=/var/lib/kubernetes/ca.crt \
  --service-account-private-key-file=/var/lib/kubernetes/service-accounts.key \
  --service-cluster-ip-range=10.32.0.0/24 \
  --use-service-account-credentials=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
----

.kube-proxy.service
[,plaintext]
----
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
----

.kube-scheduler.service
[,plaintext]
----
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --config=/etc/kubernetes/config/kube-scheduler.yaml \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
----

.kubelet.service
[,plaintext]
----
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \
  --config=/var/lib/kubelet/kubelet-config.yaml \
  --kubeconfig=/var/lib/kubelet/kubeconfig \
  --register-node=true \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
----

Create the Kubernetes configuration directory:

[,bash]
----
mkdir -p /etc/kubernetes/config
----

Install the Kubernetes Controller Binaries:

[,bash]
----
mv kube-apiserver \
    kube-controller-manager \
    kube-scheduler kubectl \
    /usr/local/bin/
----

Configure the Kubernetes API Server:

[,bash]
----
mkdir -p /var/lib/kubernetes/
mv ca.crt ca.key \
    kube-api-server.key kube-api-server.crt \
    service-accounts.key service-accounts.crt \
    encryption-config.yaml \
    /var/lib/kubernetes/
----

Create the `kube-apiserver.service` systemd unit file:

[,bash]
----
mv kube-apiserver.service /etc/systemd/system/kube-apiserver.service
----

Configure the Kubernetes Controller Manager

[,bash]
----
mv kube-controller-manager.kubeconfig /var/lib/kubernetes/
mv kube-controller-manager.service /etc/systemd/system/
----

Configure the Kubernetes Scheduler

Create the following kube-scheduler YAML definition:

.kube-scheduler.yaml
[,yaml]
----
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
----

...and move it into place.

[,bash]
----
mv kube-scheduler.kubeconfig /var/lib/kubernetes/
mv kube-scheduler.yaml /etc/kubernetes/config/
mv kube-scheduler.service /etc/systemd/system/
----

Start the controller services:

[,bash]
----
systemctl daemon-reload

systemctl enable kube-apiserver kube-controller-manager kube-scheduler

----

Check to make sure required ports are open: https://kubernetes.io/docs/reference/networking/ports-and-protocols/[Required Ports and Protocols]

.Example: Required port 6443 is not open
[,console]
----
[root@mawenzi-01 ~]# nc 127.0.0.1 6443 -zv -w 2
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connection refused.
----