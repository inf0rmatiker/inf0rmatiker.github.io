<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Machine Learning Benchmarks :: Caleb Carlson | Documentation</title>
    <link rel="canonical" href="https://inf0rmatiker.github.io/docs-site/1/learning/machine-learning/benchmarks.html">
    <meta name="generator" content="Antora 3.1.4">
    <link rel="stylesheet" href="../../../../_/css/site.css">
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://inf0rmatiker.github.io">Caleb Carlson | Documentation</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://inf0rmatiker.github.io/">Home</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="docs-site" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../../index.html">Documentation</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../index.html">Overview</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Projects</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">VU Meter</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../projects/vu-meter/vu-meter.html">VU Meter</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">PC Exhaust</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../projects/pc-exhaust/pc-exhaust.html">PC Exhaust</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Learning</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Kubernetes</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/crds.html">CRDs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/k8s-api-resources.html">Kubernetes Resources</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/k8s-install.html">Kubernetes Install</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/kubectl.html">kubectl</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Lustre</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/compiling-lustre.html">Compiling Lustre</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-client.html">Lustre Client</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-server.html">Lustre Server</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-networking.html">Lustre Networking</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Linux</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Installs</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/installs/rocky-install.html">Rocky Linux Install</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/installs/opensuse-install.html">OpenSUSE Linux Install</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/rpms.html">RPMs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/tmux.html">Tmux</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/user-management.html">User Management</a>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Networking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/linux-networking.html">Linux</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/proxies.html">Proxies</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/ssh.html">SSH</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Storage</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/filesystems.html">Filesystems</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/drives.html">Drives</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Docker</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../docker/registry-proxy.html">Registry Proxy</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Asciinema</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../asciinema/asciinema.html">Asciinema</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">BMC Management</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../bmc-management/bmc-management.html">BMC Management</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Version Control</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../version-control/git/git.html">Git</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../version-control/github/github.html">GitHub</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Algorithms</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../algorithms/algorithms.html">Algorithms</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Machine Learning</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="machine-learning.html">Machine Learning</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="determinedai.html">Determined AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">REST APIs</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../rest-apis/api-security.html">Secure Development</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">InfiniBand</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../infiniband/infiniband.html">InfiniBand</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../infiniband/monitoring.html">Fabric Monitoring</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Documentation</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../../index.html">Documentation</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../../index.html">Documentation</a></li>
    <li>Learning</li>
    <li>Machine Learning</li>
    <li><a href="benchmarks.html">Benchmarks</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Machine Learning Benchmarks</h1>
<div class="sect1">
<h2 id="_system_tuning"><a class="anchor" href="#_system_tuning"></a>System Tuning</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The binding configuration for the GPUs needs to be correct.</p>
</div>
<div class="paragraph">
<p>The idea is to minimize latency going from CPU to GPU. Typically you have NUMA domains, you specify the closest NUMA domain for each CPU.
In our system you only have half the PCI lanes, so you have to make a mapping based on cores.</p>
</div>
<div class="paragraph">
<p>See the current mapping using the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">nvidia-smi topo -m</code></pre>
</div>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">root@o186i221:~/ccarlson/experiments# nvidia-smi topo -m
	GPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	GPU7	NIC0	NIC1	NIC2	NIC3	NIC4	CPU Affinity	NUMA Affinity
GPU0	 X 	PXB	SYS	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	SYS	48-63	3
GPU1	PXB	 X 	SYS	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	SYS	48-63	3
GPU2	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	16-31	1
GPU3	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	16-31	1
GPU4	SYS	SYS	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	PXB	SYS	SYS	112-127	7
GPU5	SYS	SYS	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	PXB	SYS	SYS	112-127	7
GPU6	SYS	SYS	SYS	SYS	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	PXB	80-95	5
GPU7	SYS	SYS	SYS	SYS	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	PXB	80-95	5
NIC0	PXB	PXB	SYS	SYS	SYS	SYS	SYS	SYS	 X 	SYS	SYS	SYS	SYS
NIC1	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	SYS	 X 	SYS	SYS	SYS
NIC2	SYS	SYS	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	 X 	SYS	SYS
NIC3	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	 X 	SYS
NIC4	SYS	SYS	SYS	SYS	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	 X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks

NIC Legend:

  NIC0: mlx5_0
  NIC1: mlx5_1
  NIC2: mlx5_2
  NIC3: mlx5_3
  NIC4: mlx5_4</code></pre>
</div>
</div>
<div class="paragraph">
<p>In order to prioritize CUDA devices when building with Docker, set the <code>BUILDDOCKER_BUILDKIT</code> environment variable.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">BUILDDOCKER_BUILDKIT=1 docker build</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mlperf"><a class="anchor" href="#_mlperf"></a>MLPerf</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/">Nvidia - MLPerf</a></p>
</li>
<li>
<p><a href="https://mlcommons.org/en/">MLCommons</a></p>
</li>
</ul>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>MLPerf™ benchmarks - developed by MLCommons, a consortium of AI leaders from academia, research labs, and industry - are designed to provide unbiased evaluations of training and inference performance for hardware, software, and services. They&#8217;re all conducted under prescribed conditions. To stay on the cutting edge of industry trends, MLPerf continues to evolve, holding new tests at regular intervals and adding new workloads that represent the state of the art in AI.</p>
</div>
</blockquote>
</div>
<div class="sect2">
<h3 id="_mlperf_training"><a class="anchor" href="#_mlperf_training"></a>MLPerf Training</h3>
<div class="paragraph">
<p>MLPerf Training presents three unique benchmarking challenges missing from other domains. Optimizations that improve training throughput can
increase time to solution, training is randomly determined and time to solution exhibits high variance,
and software and hardware are diverse enough to present difficulties for fairer benchmarking with the same binary, code, and even hyperparameters.
MLPerf Training tests eight different workloads across various use cases like computer vision, large language models, and recommenders.</p>
</div>
<div class="paragraph">
<p>While the repository, <a href="https://github.com/mlcommons/training">mlcommons/training</a>, is intended to serve as valid starting points for benchmark implementations,
it is not meant to be used for real performance measurements of software frameworks and hardware.
The following steps would need to be executed in order to run a benchmark.
Speeds may vary based on the reference hardware being used.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Clone the repo <a href="https://github.com/mlcommons/training">mlcommons/training</a> to the host machine.</p>
</li>
<li>
<p>Run the <code>install_cuda_docker.sh</code> script on the host machine. This will ensure that CUDA and Docker is installed. The script would also take care of setting up the correct docker dependencies.</p>
</li>
<li>
<p>While being on the host machine and outside of docker, run <code>./download_dataset.sh</code> for the dataset being used for the benchmark. The script should be run within the directory of the benchmark.</p>
</li>
<li>
<p>Once the download completes, it can be verified by running <code>./verify_dataset.sh</code> in the same manner.</p>
</li>
<li>
<p>Build and run the docker image. Each benchmark has a command to do that.</p>
</li>
<li>
<p>Once the target quality is reached, the benchmark will stop to produce timing results.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_mlperf_storage"><a class="anchor" href="#_mlperf_storage"></a>MLPerf Storage</h3>
<div class="paragraph">
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md">MLCommons - Storage</a></p>
</div>
<div class="paragraph">
<p>MLPerf Storage is a benchmark suite to characterize the performance of storage systems that support machine learning workloads.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#overview">Overview</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#installation">Installation</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#configuration">Configuration</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#workloads">Workloads</a></p>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#u-net3d">U-Net3D</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#bert">BERT</a></p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#parameters">Parameters</a></p>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#closed">CLOSED</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#open">OPEN</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>The MLPerf Storage Benchmark Suite is an AI/ML benchmarking suite that measures performance of storage for ML workloads.
The benchmark helps bridge the gap between the utilization between storage and compute resources to make sure that they are both used efficiently for ML workloads.</p>
</div>
<div class="paragraph">
<p>It measures the sustained performance of a storage system for MLPerf Training and HPC workloads on both PyTorch and Tensorflow without requiring the use of expensive accelerators. Additionally, the <code>dlio_benchmark</code> code is used to emulate I/O patterns for deep learning workloads.</p>
</div>
<div class="sect3">
<h4 id="_accelerator_utilization"><a class="anchor" href="#_accelerator_utilization"></a>Accelerator Utilization</h4>
<div class="paragraph">
<p>The Accelerator Utilization (AU) is the benchmark output metric samples per second for each workload.
In order the calculate the AU, the total compute time and the total benchmark running time is needed.
The total compute time is calculated by:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">total_compute_time = (records/file * total_files)/simulated_accelerators/batch_size * sleep_time</code></pre>
</div>
</div>
<div class="paragraph">
<p>The formula below calculates the AU:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">AU (percentage) = (total_compute_time/total_benchmark_running_time) * 100</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_mlperf_storage_installation"><a class="anchor" href="#_mlperf_storage_installation"></a>MLPerf Storage Installation</h4>
<div class="paragraph">
<p>First, the <code>mpich</code> for MPI package is needed. This can be done by:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo apt-get install mpich</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, clone the <a href="https://github.com/mlcommons/storage">MLCommons Storage</a> repo.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">git clone https://github.com/mlcommons/storage.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>requirements.txt</code> for <code>dlio_benchmark</code> would need to be installed. Before that, a Python virtual environment needs to be set up:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">python3 -m venv ~/myvenv
source ~/myvenv/bin/activate</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>requirements.txt</code> would then be installed afterwards.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">pip install -r dlio_benchmark/requirements.txt</code></pre>
</div>
</div>
<div class="paragraph">
<p>To launch the <code>dlio_benchmark</code>, execute the <code>benchmark.sh</code> script:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">./benchmark.sh -h</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_resnet_50"><a class="anchor" href="#_resnet_50"></a>RESNET-50</h3>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/training/blob/master/object_detection/README.md">RESNET-50 Object Detection Instructions</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/training_results_v3.0/tree/main/NVIDIA/benchmarks/resnet/implementations/mxnet">Nvidia RESNET Instructions</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The RESNET-50 neural network is a well-known image classification network that can be used with the <code>ImageNet</code> dataset. It computationally intensive and good indication of driving meaningful storage I/O. In order to keep up the training benchmark and the overall run average time, the storage system has to keep up with the read bandwidth demands of a complete training job. The training benchmark are measured at <code>Epoch 0</code>, which is the most I/O-intensive portion of the MLPerf benchmark run. On that note, the RESNET-50 test would verify if the storage system is not a bottleneck for the workload and will provide the same images/second for <code>Epoch 0</code>.</p>
</div>
<div class="sect3">
<h4 id="_resnet_50_installation"><a class="anchor" href="#_resnet_50_installation"></a>RESNET-50 Installation</h4>
<div class="paragraph">
<p>In order to run the MLPerf RESNET-50 test, <a href="https://github.com/mlcommons/ck/tree/master/cm/cmind">Collective Mind automation language</a> (CM) (also known as ML Commons CM language)  would need to be installed. It is part of the MLCommons Collective Knowledge (CK) project, and is powered by Python, JSON, YAML, and a unified CLI. More detailed information on CM  can be found here: <a href="https://github.com/mlcommons/ck/tree/master/cm#readme">Collective Minds</a>. The following installation steps assume the host machine will be running on RedHat Enterprise Linux. Furthermore, <code>python 3+</code>, <code>pip</code>, <code>git</code> , and <code>wget</code>  would need to be installed beforehand.</p>
</div>
<div class="paragraph">
<p>The following will install CM:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo dnf update
sudo dnf install python3 python-pip git wget curl
python3 -m pip install --user cmind</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once CM is installed, the next step would be to install MLCommons CK repository with automation workflows for MLPerf:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cm pull repo mlcommons@ck</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command will pre-process the dataset for a given backend. The <code>loadgen</code> would be built, and it will run the inference for all scenarios and modes.
A submission folder would be created with the test results.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cmr "run mlperf inference generate-run-cmds _submission" \
    --quiet --submitter="MLCommons" --hw_name=default --model=resnet50 --implementation=reference \
    --backend=onnxruntime --device=cpu --scenario=Offline --adr.compiler.tags=gcc  --target_qps=1 \
    --category=edge --division=open</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>target_qps</code> value would need to be updated according to the system performance for a valid submission.
The following values should be as is,</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use <code>--device=cuda</code> to run the inference on Nvidia GPU</p>
</li>
<li>
<p>Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode)</p>
</li>
<li>
<p>Use <code>--category=datacenter</code> to run datacenter scenarios</p>
</li>
<li>
<p>Use <code>--backend=tf</code> or <code>--backend=tvm-onnx</code> to use <code>tensorflow</code> and <code>tvm-onnx</code> backends, respectively</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Credit goes to <a href="https://www.linkedin.com/in/sakib-samar-23a79612a">Sakib Samar</a> for a large portion of these notes.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_nccl_tests"><a class="anchor" href="#_nccl_tests"></a>NCCL Tests</h3>
<div class="paragraph">
<p>These tests check both the performance and the correctness of <a href="https://github.com/nvidia/nccl">NCCL</a> operations (inter-GPU communication).</p>
</div>
<div class="paragraph">
<p>GitHub repositories:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/NVIDIA/nccl-tests/tree/master">nccl-tests</a></p>
</li>
<li>
<p><a href="https://github.com/nvidia/nccl">nccl</a></p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_installation"><a class="anchor" href="#_installation"></a>Installation</h4>
<div class="paragraph">
<p>First, you need <code>nccl</code> installed on the nodes you want to include in the benchmark.</p>
</div>
<div class="paragraph">
<p>Go ahead and clone the <code>nccl</code> repo:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">git clone https://github.com/NVIDIA/nccl.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>Change directory into the <code>nccl</code> repo and build it against your system:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cd nccl
make -j src.build</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now, install it (here we&#8217;re using Ubuntu 22.04):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Install tools to create debian packages
sudo apt install build-essential devscripts debhelper fakeroot
# Build NCCL deb package
make pkg.debian.build
ls build/pkg/deb/</code></pre>
</div>
</div>
<div class="paragraph">
<p>Your <code>.deb</code> packaages should now be in <code>build/pkg/deb</code>. You can install them using</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">apt install /root/ccarlson/nccl/build/pkg/deb/libnccl-dev_2.18.5-1+cuda12.2_amd64.deb /root/ccarlson/nccl/build/pkg/deb/libnccl2_2.18.5-1+cuda12.2_amd64.deb</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, clone your <code>nccl-tests</code> repo:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">git clone https://github.com/NVIDIA/nccl-tests.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now, change directory into it and <code>make</code> it against the installation of MPI you have on the system:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cd nccl-tests
make MPI=1 MPI_HOME=/usr/mpi/gcc/openmpi-4.1.5rc2</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should now have <code>nccl-tests</code> binaries available under <code>build/</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">root@o186i221:~/ccarlson/nccl-tests# ls build/
all_gather_perf  alltoall_perf   gather_perf     reduce_perf          scatter_perf   timer.o
all_reduce_perf  broadcast_perf  hypercube_perf  reduce_scatter_perf  sendrecv_perf  verifiable</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_usage"><a class="anchor" href="#_usage"></a>Usage</h4>
<div class="paragraph">
<p><a href="https://github.com/NVIDIA/nccl-tests/tree/master#arguments">NCCL Tests Arguments</a></p>
</div>
<div class="paragraph">
<p><code>all_reduce_perf</code> options:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">USAGE: all_reduce_perf
	[-t,--nthreads &lt;num threads&gt;]
	[-g,--ngpus &lt;gpus per thread&gt;]
	[-b,--minbytes &lt;min size in bytes&gt;]
	[-e,--maxbytes &lt;max size in bytes&gt;]
	[-i,--stepbytes &lt;increment size&gt;]
	[-f,--stepfactor &lt;increment factor&gt;]
	[-n,--iters &lt;iteration count&gt;]
	[-m,--agg_iters &lt;aggregated iteration count&gt;]
	[-w,--warmup_iters &lt;warmup iteration count&gt;]
	[-p,--parallel_init &lt;0/1&gt;]
	[-c,--check &lt;check iteration count&gt;]
	[-o,--op &lt;sum/prod/min/max/avg/mulsum/all&gt;]
	[-d,--datatype &lt;nccltype/all&gt;]
	[-r,--root &lt;root&gt;]
	[-z,--blocking &lt;0/1&gt;]
	[-y,--stream_null &lt;0/1&gt;]
	[-T,--timeout &lt;time in seconds&gt;]
	[-G,--cudagraph &lt;num graph launches&gt;]
	[-C,--report_cputime &lt;0/1&gt;]
	[-a,--average &lt;0/1/2/3&gt; report average iteration time &lt;0=RANK0/1=AVG/2=MIN/3=MAX&gt;]
	[-h,--help]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Running the NCCL tests on a single node is pretty straightforward:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">/root/ccarlson/nccl-tests/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1</code></pre>
</div>
</div>
<div class="paragraph">
<p>Running across multiple nodes should be done with MPI (<code>mpirun</code>)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">mpirun --allow-run-as-root -np 2 --mca btl_tcp_if_include eth0 --machinefile machinefile.txt --map-by "node" /root/ccarlson/nccl-tests/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1</code></pre>
</div>
</div>
<div class="paragraph">
<p>My <code>machinefile</code> for two nodes looks like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">o186i221 slots=64
o186i222 slots=64</code></pre>
</div>
</div>
<div class="paragraph">
<p>This launches two tasks via MPI (<code>-np 2</code>), one per node, each of which runs the <code>all_reduce_perf</code> binary with the specified options.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
We have to include the <code>eth0</code> interface for MPI to communicate over, otherwise it&#8217;ll try to send TCP over the IB devices which won&#8217;t work.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Example output for multiple nodes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1
# nThread 1 nGpus 8 minBytes 134217728 maxBytes 1073741824 step: 2(factor) warmup iters: 5 iters: 1 agg iters: 1 validation: 1 graph: 0
#
# Using devices
#  Rank  0 Group  0 Pid 256912 on   o186i221 device  0 [0x07] NVIDIA A100-SXM4-80GB
#  Rank  1 Group  0 Pid 256912 on   o186i221 device  1 [0x0b] NVIDIA A100-SXM4-80GB
#  Rank  2 Group  0 Pid 256912 on   o186i221 device  2 [0x48] NVIDIA A100-SXM4-80GB
#  Rank  3 Group  0 Pid 256912 on   o186i221 device  3 [0x4c] NVIDIA A100-SXM4-80GB
#  Rank  4 Group  0 Pid 256912 on   o186i221 device  4 [0x88] NVIDIA A100-SXM4-80GB
#  Rank  5 Group  0 Pid 256912 on   o186i221 device  5 [0x8b] NVIDIA A100-SXM4-80GB
#  Rank  6 Group  0 Pid 256912 on   o186i221 device  6 [0xc8] NVIDIA A100-SXM4-80GB
#  Rank  7 Group  0 Pid 256912 on   o186i221 device  7 [0xcb] NVIDIA A100-SXM4-80GB
#  Rank  8 Group  0 Pid 540236 on   o186i222 device  0 [0x07] NVIDIA A100-SXM4-80GB
#  Rank  9 Group  0 Pid 540236 on   o186i222 device  1 [0x0b] NVIDIA A100-SXM4-80GB
#  Rank 10 Group  0 Pid 540236 on   o186i222 device  2 [0x48] NVIDIA A100-SXM4-80GB
#  Rank 11 Group  0 Pid 540236 on   o186i222 device  3 [0x4c] NVIDIA A100-SXM4-80GB
#  Rank 12 Group  0 Pid 540236 on   o186i222 device  4 [0x88] NVIDIA A100-SXM4-80GB
#  Rank 13 Group  0 Pid 540236 on   o186i222 device  5 [0x8b] NVIDIA A100-SXM4-80GB
#  Rank 14 Group  0 Pid 540236 on   o186i222 device  6 [0xc8] NVIDIA A100-SXM4-80GB
#  Rank 15 Group  0 Pid 540236 on   o186i222 device  7 [0xcb] NVIDIA A100-SXM4-80GB
#
#                                                              out-of-place                       in-place
#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
   134217728      33554432     float     sum      -1   5619.6   23.88   44.78      0   5662.5   23.70   44.44      0
   268435456      67108864     float     sum      -1    10796   24.86   46.62      0    10836   24.77   46.45      0
   536870912     134217728     float     sum      -1    14128   38.00   71.25      0    13863   38.73   72.61      0
  1073741824     268435456     float     sum      -1    24419   43.97   82.45      0    24255   44.27   83.00      0
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 61.4514
#</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <p>Copyright © 2023 Caleb Carlson. All rights reserved.
</footer>
<script id="site-script" src="../../../../_/js/site.js" data-ui-root-path="../../../../_"></script>
<script async src="../../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
