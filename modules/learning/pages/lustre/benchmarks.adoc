= Lustre Filesystem Benchmarks

:toc: auto
:showtitle:

== FIO

https://fio.readthedocs.io/en/latest/fio_doc.html[FIO Documentation]

Usage:

[,console]
----
fio --name benchmark1 --directory=/lus/aiholus1/disk/ccarlson/ --group_reporting --rw=read --size=16g --iodepth=128 --blocksize=1024k --direct=1 --numjobs=128 --ioengine=libaio | tee results_max2.out
----

== IOR

Documentation:

* https://wiki.lustre.org/IOR[IOR Documentation]
* https://wiki.lustre.org/IOR#Download_and_Compile_IOR[Download and Compile IOR]
* https://ior.readthedocs.io/en/latest/userDoc/options.html[IOR Command Options]

=== Installation

* https://ior.readthedocs.io/en/latest/userDoc/install.html[`ior` Installation Documentation]

Stop your firewall, this can prevent openmpi communication:

[,bash]
----
systemctl stop firewalld
setenforce 0
----

Install openmpi for your distribution:

* https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html[Installing OpenMPI Quickstart]

Clone the `ior` repo:

[,bash]
----
git clone https://github.com/hpc/ior
cd ior/
----

Next, bootstrap, configure and make the IOR software:

[,bash]
----
./bootstrap
./configure
make
----

Add the built `ior` binary to your `PATH` variable:

[,bash]
----
export PATH="$PATH:$HOME/ior/src/ior"
----

=== Usage

1. Login to one of the compute nodes as the benchmark user
2. Create a host file for the `mpirun` command, containing the list of Lustre clients that will be used for the benchmark. Each line in the file represents a machine and the number of slots (usually equal to the number of CPU cores). For example:
+
[,bash]
----
for i in $(seq -f "%02g" 1 4); do
  echo "n"$i" slots=128"
done > $HOME/hostfile
----
+
This results in a file like the following:
+
----
n01 slots=128
n02 slots=128
n03 slots=128
n04 slots=128
----
+
* The first column of the host file contains the name of the nodes. This can also be an IP address if the /etc/hosts file or DNS is not set up.
* The second column is used to represent the number of CPU cores.

3. Run a quick test using `mpirun` to launch the benchmark and verify that the environment is set up correctly. For example:
+
[,bash]
----
mpirun --hostfile $HOME/hostfile --map-by node -np $(cat $HOME/hostfile | wc -l) hostname
----

If this is working, you can move on to using the IOR tool. I use the following `benchmark.sh` script to make benchmarks easier 
to execute by just changing some variables:

[,bash]
----
#!/bin/bash

set -ex

IOR_BIN="/home/ccarlson/ior/src/ior"
MACHINE_FILE="machinefile.txt"
TOTAL_HOSTS=$(cat $MACHINE_FILE | wc -l)
THREADS_PER_NODE=(1 8 16 32 64 128)
BLOCKSIZE_PER_TASK="16g"
TRANSFER_SIZES=("128k" "1m" "16m" "64m")
TEST_DIRECTORY="/mnt/cstor1/ccarlson/flash"
TEST_FILE="testfile"

# Sequential Write
function seq_write {
  TRANSFER_SIZE=$1
  THREADS=$2
  OUT_FILE="$OUT_DIR/seq_write_${TRANSFER_SIZE}_${THREADS}.csv"
  mpirun --allow-run-as-root --machinefile $MACHINE_FILE --mca btl_tcp_if_include eth0 -np $(( THREADS * TOTAL_HOSTS )) --map-by "node" \
    $IOR_BIN -v -F --posix.odirect -C -t $TRANSFER_SIZE -b $BLOCKSIZE_PER_TASK -e -w -k -o $TEST_DIRECTORY/$TEST_FILE \
    -O summaryFile=$OUT_FILE -O summaryFormat=CSV
  RESULTS=$(tail -1 $OUT_FILE)
  echo -e "seq_write,$THREADS,$TRANSFER_SIZE,$RESULTS" >> $OUT_DIR/results.csv
}

# Random Write
function rand_write {
  TRANSFER_SIZE=$1
  THREADS=$2
  OUT_FILE="$OUT_DIR/rand_write_${TRANSFER_SIZE}_${THREADS}.csv"
  mpirun --allow-run-as-root --machinefile $MACHINE_FILE --mca btl_tcp_if_include eth0 -np $(( THREADS * TOTAL_HOSTS )) --map-by "node" \
    $IOR_BIN -v -F --posix.odirect -C -t $TRANSFER_SIZE -b $BLOCKSIZE_PER_TASK -e -w -z -k -o $TEST_DIRECTORY/$TEST_FILE \
    -O summaryFile=$OUT_FILE -O summaryFormat=CSV
  RESULTS=$(tail -1 $OUT_FILE)
  echo -e "rand_write,$THREADS,$TRANSFER_SIZE,$RESULTS" >> $OUT_DIR/results.csv
}

# Sequential Read
function seq_read {
  TRANSFER_SIZE=$1
  THREADS=$2
  OUT_FILE="$OUT_DIR/seq_read_${TRANSFER_SIZE}_${THREADS}.csv"
  mpirun --allow-run-as-root --machinefile $MACHINE_FILE --mca btl_tcp_if_include eth0 -np $(( THREADS * TOTAL_HOSTS )) --map-by "node" \
    $IOR_BIN -v -F --posix.odirect -C -t $TRANSFER_SIZE -b $BLOCKSIZE_PER_TASK -r -k -o $TEST_DIRECTORY/$TEST_FILE \
    -O summaryFile=$OUT_FILE -O summaryFormat=CSV
  RESULTS=$(tail -1 $OUT_FILE)
  echo -e "seq_read,$THREADS,$TRANSFER_SIZE,$RESULTS" >> $OUT_DIR/results.csv
}

# Random Read
function rand_read {
  TRANSFER_SIZE=$1
  THREADS=$2
  OUT_FILE="$OUT_DIR/rand_read_${TRANSFER_SIZE}_${THREADS}.csv"
  mpirun --allow-run-as-root --machinefile $MACHINE_FILE --mca btl_tcp_if_include eth0 -np $(( THREADS * TOTAL_HOSTS )) --map-by "node" \
    $IOR_BIN -v -F --posix.odirect -C -t $TRANSFER_SIZE -b $BLOCKSIZE_PER_TASK -r -z -k -o $TEST_DIRECTORY/$TEST_FILE \
    -O summaryFile=$OUT_FILE -O summaryFormat=CSV
  RESULTS=$(tail -1 $OUT_FILE)
  echo -e "rand_read,$THREADS,$TRANSFER_SIZE,$RESULTS" >> $OUT_DIR/results.csv
}

[ $# -ne 1 ] && echo -e "Usage:\n\tbenchmark.sh <output_directory>\n" && exit 1

OUT_DIR=$1
mkdir -p $OUT_DIR

echo -e "access_type,threads,transfer_size,access,bw(MiB/s),IOPS,Latency,block(KiB),xfer(KiB),open(s),wr/rd(s),close(s),total(s),numTasks,iter" \
  > $OUT_DIR/results.csv

for THREADS in ${THREADS_PER_NODE[@]}; do
  for TRANSFER_SIZE in ${TRANSFER_SIZES[@]}; do
    echo -e "\nRunning benchmark with transfer size of $TRANSFER_SIZE, and $THREADS threads\n"
    seq_write $TRANSFER_SIZE $THREADS
    seq_read $TRANSFER_SIZE $THREADS
    rand_write $TRANSFER_SIZE $THREADS
    rand_read $TRANSFER_SIZE $THREADS
  done
done

----

You can use this by just running `./benchmark.sh <output_directory>`, i.e:

[,bash]
----
./benchmark.sh /home/ccarlson/multi_node
----

This will collect all your aggregated CSV results into a single `results.csv` file, a snippet of which looks like:

[,csv]
----
access_type,threads,transfer_size,access,bw(MiB/s),IOPS,Latency,block(KiB),xfer(KiB),open(s),wr/rd(s),close(s),total(s),numTasks,iter
seq_write,32,16m,write,27135.9688,1696.0195,0.0363,16777216.0000,16384.0000,0.0086,38.6411,24.2267,38.6416,64,0
seq_read,32,16m,read,28805.2367,1800.3649,0.0355,16777216.0000,16384.0000,0.0016,36.4015,24.7580,36.4023,64,0
rand_write,32,16m,write,37879.2400,2367.4950,0.0257,16777216.0000,16384.0000,0.0114,27.6816,2.0832,27.6821,64,0
rand_read,32,16m,read,39275.2087,2454.7904,0.0257,16777216.0000,16384.0000,0.0017,26.6972,3.4322,26.6982,64,0
seq_write,32,64m,write,41838.6043,653.7421,0.0746,16777216.0000,65536.0000,0.0090,25.0619,6.8782,25.0624,64,0
seq_read,32,64m,read,43203.6925,675.0791,0.0912,16777216.0000,65536.0000,0.0015,24.2697,7.1871,24.2705,64,0
rand_write,32,64m,write,40580.2781,634.0792,0.0908,16777216.0000,65536.0000,0.1828,25.8390,8.8996,25.8395,64,0
rand_read,32,64m,read,43326.2159,676.9922,0.0713,16777216.0000,65536.0000,0.0024,24.2012,7.6495,24.2019,64,0
----

=== Usage Example

The following example uses `mpirun` to execute a single instance (`-np 1`) of `ior` on the machine provided in `machinefile.txt`, mapped by slots available on that machine.
IOR is doing a sequential write (`-w`) benchmark, using file-per-process (`-F`), bypasses the hosts buffer with ODIRECT=1 flag (`--posix.odirect=1`), reordering tasks (`-C`),
with a transfer size of 128 KiB at a time (`-t 128k`), and a total block size of 16 GiB (`-b 16g`) per process. It does an fsync after the write operation is closed (`-e`), and keeps the files it wrote (`-k`). The output files go to `/mnt/cstor1/ccarlson/testfile.XXXX` where `XXXX` is the process ID. Finally, the benchmark summary is output to the file
`single_node/slots_1/seq_write_128k.csv` in the CSV format.

[,bash]
----
mpirun --allow-run-as-root --machinefile machinefile.txt -np 1 --map-by slot \
  /home/ccarlson/ior/src/ior -v \
    -F --posix.odirect -C -t 128k -b 16g -e -w -k \
    -o /mnt/cstor1/ccarlson/testfile \
    -O summaryFile=single_node/slots_1/seq_write_128k.csv \
    -O summaryFormat=CSV
----

=== Command-line Options

* https://ior.readthedocs.io/en/latest/userDoc/options.html[IOR Command Options]

[cols="1,4"]
|===
| Option | Description

|-a S
|api - API for I/O [POSIX\|MPIIO\|HDF5\|HDFS\|S3\|S3_EMC\|NCMPI\|RADOS]

|-A N
|refNum - user reference number to include in long summary

|-b N
|blockSize - contiguous bytes to write per task (e.g.: 8, 4k, 2m, 1g)

|-c
|collective - collective I/O

|-C
|reorderTasksConstant - changes task ordering to n+1 ordering for readback

|-d N
|interTestDelay - delay between reps in seconds

|-D N
|deadlineForStonewalling - seconds before stopping write or read phase

|-e
|fsync - perform fsync upon POSIX write close

|-E
|useExistingTestFile - do not remove test file before write access

|-f S
|scriptFile - test script name

|-F
|filePerProc - file-per-process

|-g
|intraTestBarriers - use barriers between open, write/read, and close

|-G N
|setTimeStampSignature - set value for time stamp signature

|-h
|showHelp - displays options and help

|-H
|showHints - show hints

|-i N
|repetitions - number of repetitions of test

|-I
|individualDataSets - datasets not shared by all procs [not working]

|-j N
|outlierThreshold - warn on outlier N seconds from mean
	
|-J N
|setAlignment - HDF5 alignment in bytes (e.g.: 8, 4k, 2m, 1g)

|-k
|keepFile - don't remove the test file(s) on program exit
	
|-K
|keepFileWithError - keep error-filled file(s) after data-checking
	
|-l
|data packet type- type of packet that will be created [offset\|incompressible\|timestamp\|o\|i\|t]
	
|-m
|multiFile - use number of reps (-i) for multiple file count

|-M N
|memoryPerNode - hog memory on the node (e.g.: 2g, 75%)

|-n
|noFill - no fill in HDF5 file creation

|-N N
|numTasks - number of tasks that should participate in the test

|-o S
|testFile - full name for test

|-O S
|string of IOR directives (e.g. -O checkRead=1,GPUid=2)

|-p
|preallocate - preallocate file size

|-P
|useSharedFilePointer - use shared file pointer [not working]

|-q
|quitOnError - during file error-checking, abort on error

|-Q N
|taskPerNodeOffset for read tests use with -C & -Z options (-C constant N, -Z at least N) [!HDF5]

|-r
|readFile - read existing file

|-R
|checkRead - check read after read

|-s N
|segmentCount - number of segments

|-S
|useStridedDatatype - put strided access into datatype [not working]

|-t N
|transferSize - size of transfer in bytes (e.g.: 8, 4k, 2m, 1g)

|-T N
|maxTimeDuration - max time in minutes to run tests

|-u
|uniqueDir - use unique directory name for each file-per-process

|-U S
|hintsFileName - full name for hints file

|-v
|verbose - output information (repeating flag increases level)

|-V
|useFileView - use MPI_File_set_view

|-w
|writeFile - write file

|-W
|checkWrite - check read after write

|-x
|singleXferAttempt - do not retry transfer if incomplete

|-X N
|reorderTasksRandomSeed - random seed for -Z option

|-Y
|fsyncPerWrite - perform fsync after each POSIX write

|-z	
|randomOffset - access is to random, not sequential, offsets within a file

|-Z
|reorderTasksRandom - changes task ordering to random ordering for readback
|===

* *S* is a string, *N* is an integer number.
* For transfer and block sizes, the case-insensitive *K*, *M*, and *G* suffices are recognized. I.e., `4k` or `4K` is accepted as 4096.

=== Overview of IOR Benchmarks with System Monitoring

video::FM7a9HuOl-k?si=1hpPs0SM7Ds2uQM-[youtube,width=960,height=440]

== Case Study: Grenoble System Benchmark Results

Here we show a demo of the benchmark results captured using the aforementioned tools on the flash pool of a single ClusterStor E1000.

=== Single-node Performance

MPI parameters:

* Number of processes: 1, 16, 32, 64, 128
* Nodes: 1
* Map-by: slots on node (node capable of 128 slots)

IOR write parameters:

* File-per-process (`-F`)
* POSIX write directives: O_DIRECT (`--posix.odirect`)
* Transfer sizes: 128k, 1m, 16m, 64m (`-t N`)
* Blocksize per task: 16g (`-b N`)
* Invoke fsync on POSIX write close (`-e`)
* Keep written files for reading later (`-k`)

Additionally, the `-z` flag was used for the random writes test to write to random offsets.

IOR read parameters:

* File-per-process (`-F`)
* Transfer sizes: 128k, 1m, 16m, 64m (`-t N`)
* Shift reads to what our node _didn't_ write, if we have neighboring nodes (`-C`)
* Blocksize per task: 16g (`-b N`)

Additionally, the `-z` flag was used for the random reads test to read from random offsets.

=== Single-node Plotted Results

Visualizing these results with `matplotlib` shows us some critical information:

*Figure 1: Single-node write throughput*: _Write performance varies by concurrent thread count, categorized by transfer size._

image::docs-site:learning:image$lustre/grenoble_ior_write_perf.png[Grenoble Write Performance]

*Figure 2: Single-node write IOPS*: _Write performance varies by concurrent thread count, categorized by transfer size._

image::docs-site:learning:image$lustre/grenoble_ior_write_iops.png[Grenoble Write IOPS]

*Figure 3: Single-node read throughput*: _Read performance varies by concurrent thread count, categorized by transfer size._

image::docs-site:learning:image$lustre/grenoble_ior_read_perf.png[Grenoble Read Performance]

*Figure 4: Single-node read IOPS*: _Read performance varies by concurrent thread count, categorized by transfer size._

image::docs-site:learning:image$lustre/grenoble_ior_read_iops.png[Grenoble Read IOPS]

From the figures above, we can see that transfer size is inversely correlated with IOPS; the bigger your transfers are, the fewer you can do per second.
Lustre has higher throughput with larger transfer sizes, meaning higher throughput means lower IOPS. We can also see the effects of having tuned the system
for large I/O transfer sizes, in our case, 64 MiB transfer sizes. Trying to use smaller 128 KiB transfer sizes with a system tuned this way simply does not perform
well when it comes to write throughput. Another curious discovery is that 1 MiB transfer sizes actually beat 128 KiB transfer sizes in terms of IOPS; a theory here is that
Lustre queues up several 128 KiB transactions before sending, whereas 1 MiB transactions are sent immediately. Again, this is a tuning option that can be configured differently
based on the user's needs.
