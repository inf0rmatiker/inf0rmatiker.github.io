<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Machine Learning Benchmarks :: Caleb Carlson | Docs Site</title>
    <link rel="canonical" href="https://inf0rmatiker.github.io/docs-site/1/learning/machine-learning/benchmarks.html">
    <meta name="generator" content="Antora 3.1.4">
    <link rel="stylesheet" href="../../../../_/css/site.css">
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://inf0rmatiker.github.io">Caleb Carlson | Docs Site</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://inf0rmatiker.github.io/">Home</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="docs-site" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../../index.html">Documentation</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../index.html">Overview</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Projects</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">VU Meter</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../projects/vu-meter/vu-meter.html">VU Meter</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">PC Exhaust</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../projects/pc-exhaust/pc-exhaust.html">PC Exhaust</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Learning</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Kubernetes</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/crds.html">CRDs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/k8s-api-resources.html">Kubernetes Resources</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/k8s-install.html">Kubernetes Install</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/kubectl.html">kubectl</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Lustre</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/compiling-lustre.html">Compiling Lustre</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/development.html">Development</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-client.html">Lustre Client</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-server.html">Lustre Server</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-networking.html">Lustre Networking</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Linux</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Installs</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/installs/rocky-install.html">Rocky Linux Install</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/installs/opensuse-install.html">OpenSUSE Linux Install</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/isos.html">ISOs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/rpms.html">RPMs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/tmux.html">Tmux</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/user-management.html">User Management</a>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Networking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/linux-networking.html">Linux</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/mpi.html">MPI</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/proxies.html">Proxies</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/rdma.html">RDMA Programming</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/socket_programming.html">Socket Programming</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/ssh.html">SSH</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Storage</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/filesystems.html">Filesystems</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/drives.html">Drives</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Docker</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../docker/multiarch-building.html">Multi-Architecture Builds</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../docker/nginx-webserver.html">Nginx Webserver</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../docker/registry-proxy.html">Registry Proxy</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../asciinema/asciinema.html">Asciinema</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../bmc-management/bmc-management.html">BMC Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Version Control</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../version-control/git/git.html">Git</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../version-control/github/github.html">GitHub</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../algorithms/algorithms.html">Algorithms</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Machine Learning</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="determinedai.html">Determined AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">REST APIs</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../rest-apis/api-security.html">Secure Development</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">InfiniBand</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../infiniband/infiniband.html">InfiniBand</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../infiniband/monitoring.html">Fabric Monitoring</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Languages</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">C</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/c/getopt.html">Getopt</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../languages/cpp/cpp.html">C++</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/arrays.html">Arrays</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/branch_control.html">Branch Control</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/classes.html">Classes</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/compiling.html">Compiling and Executing</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/development_environment.html">Development Environment</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/error_handling.html">Error Handling</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/functions.html">Functions</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/input_streams.html">Input Streams</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/macros.html">Macros</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/pointers.html">Pointers</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/references.html">References</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/structures.html">Stuctures</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/types.html">Types</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../languages/cpp/vectors.html">Vectors</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../languages/go/go.html">Go</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../slurm/slurm.html">Slurm</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">GPUs</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../gpus/gds.html">GPUDirect Storage</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../gpus/numa.html">NUMA</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../switch-management/switch-management.html">Switch Management</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../hpcm/hpcm.html">HPCM</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Documentation</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../../index.html">Documentation</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../../index.html">Documentation</a></li>
    <li>Learning</li>
    <li>Machine Learning</li>
    <li><a href="benchmarks.html">Benchmarks</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Machine Learning Benchmarks</h1>
<div class="sect1">
<h2 id="_system_tuning"><a class="anchor" href="#_system_tuning"></a>System Tuning</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The binding configuration for the GPUs needs to be correct.</p>
</div>
<div class="paragraph">
<p>The idea is to minimize latency going from CPU to GPU. Typically you have NUMA domains, you specify the closest NUMA domain for each CPU.
In our system you only have half the PCI lanes, so you have to make a mapping based on cores.</p>
</div>
<div class="paragraph">
<p>See the current mapping using the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">nvidia-smi topo -m</code></pre>
</div>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">root@o186i221:~/ccarlson/experiments# nvidia-smi topo -m
	GPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	GPU7	NIC0	NIC1	NIC2	NIC3	NIC4	CPU Affinity	NUMA Affinity
GPU0	 X 	PXB	SYS	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	SYS	48-63	3
GPU1	PXB	 X 	SYS	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	SYS	48-63	3
GPU2	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	16-31	1
GPU3	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	16-31	1
GPU4	SYS	SYS	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	PXB	SYS	SYS	112-127	7
GPU5	SYS	SYS	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	PXB	SYS	SYS	112-127	7
GPU6	SYS	SYS	SYS	SYS	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	PXB	80-95	5
GPU7	SYS	SYS	SYS	SYS	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	PXB	80-95	5
NIC0	PXB	PXB	SYS	SYS	SYS	SYS	SYS	SYS	 X 	SYS	SYS	SYS	SYS
NIC1	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	SYS	 X 	SYS	SYS	SYS
NIC2	SYS	SYS	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	 X 	SYS	SYS
NIC3	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	 X 	SYS
NIC4	SYS	SYS	SYS	SYS	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	 X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks

NIC Legend:

  NIC0: mlx5_0
  NIC1: mlx5_1
  NIC2: mlx5_2
  NIC3: mlx5_3
  NIC4: mlx5_4</code></pre>
</div>
</div>
<div class="paragraph">
<p>In order to prioritize CUDA devices when building with Docker, set the <code>BUILDDOCKER_BUILDKIT</code> environment variable.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">BUILDDOCKER_BUILDKIT=1 docker build</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mlperf"><a class="anchor" href="#_mlperf"></a>MLPerf</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/">Nvidia - MLPerf</a></p>
</li>
<li>
<p><a href="https://mlcommons.org/en/">MLCommons</a></p>
</li>
</ul>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>MLPerf™ benchmarks - developed by MLCommons, a consortium of AI leaders from academia, research labs, and industry - are designed to provide unbiased evaluations of training and inference performance for hardware, software, and services. They&#8217;re all conducted under prescribed conditions. To stay on the cutting edge of industry trends, MLPerf continues to evolve, holding new tests at regular intervals and adding new workloads that represent the state of the art in AI.</p>
</div>
</blockquote>
</div>
<div class="sect2">
<h3 id="_mlperf_training"><a class="anchor" href="#_mlperf_training"></a>MLPerf Training</h3>
<div class="paragraph">
<p>MLPerf Training presents three unique benchmarking challenges missing from other domains. Optimizations that improve training throughput can
increase time to solution, training is randomly determined and time to solution exhibits high variance,
and software and hardware are diverse enough to present difficulties for fairer benchmarking with the same binary, code, and even hyperparameters.
MLPerf Training tests eight different workloads across various use cases like computer vision, large language models, and recommenders.</p>
</div>
<div class="paragraph">
<p>While the repository, <a href="https://github.com/mlcommons/training">mlcommons/training</a>, is intended to serve as valid starting points for benchmark implementations,
it is not meant to be used for real performance measurements of software frameworks and hardware.
The following steps would need to be executed in order to run a benchmark.
Speeds may vary based on the reference hardware being used.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Clone the repo <a href="https://github.com/mlcommons/training">mlcommons/training</a> to the host machine.</p>
</li>
<li>
<p>Run the <code>install_cuda_docker.sh</code> script on the host machine. This will ensure that CUDA and Docker is installed. The script would also take care of setting up the correct docker dependencies.</p>
</li>
<li>
<p>While being on the host machine and outside of docker, run <code>./download_dataset.sh</code> for the dataset being used for the benchmark. The script should be run within the directory of the benchmark.</p>
</li>
<li>
<p>Once the download completes, it can be verified by running <code>./verify_dataset.sh</code> in the same manner.</p>
</li>
<li>
<p>Build and run the docker image. Each benchmark has a command to do that.</p>
</li>
<li>
<p>Once the target quality is reached, the benchmark will stop to produce timing results.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_mlperf_storage"><a class="anchor" href="#_mlperf_storage"></a>MLPerf Storage</h3>
<div class="paragraph">
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md">MLCommons - Storage</a></p>
</div>
<div class="paragraph">
<p>MLPerf Storage is a benchmark suite to characterize the performance of storage systems that support machine learning workloads.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#overview">Overview</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#installation">Installation</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#configuration">Configuration</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#workloads">Workloads</a></p>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#u-net3d">U-Net3D</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#bert">BERT</a></p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#parameters">Parameters</a></p>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#closed">CLOSED</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#open">OPEN</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>The MLPerf Storage Benchmark Suite is an AI/ML benchmarking suite that measures performance of storage for ML workloads.
The benchmark helps bridge the gap between the utilization between storage and compute resources to make sure that they are both used efficiently for ML workloads.</p>
</div>
<div class="paragraph">
<p>It measures the sustained performance of a storage system for MLPerf Training and HPC workloads on both PyTorch and Tensorflow without requiring the use of expensive accelerators. Additionally, the <code>dlio_benchmark</code> code is used to emulate I/O patterns for deep learning workloads.</p>
</div>
<div class="sect3">
<h4 id="_accelerator_utilization"><a class="anchor" href="#_accelerator_utilization"></a>Accelerator Utilization</h4>
<div class="paragraph">
<p>The Accelerator Utilization (AU) is the benchmark output metric samples per second for each workload.
In order the calculate the AU, the total compute time and the total benchmark running time is needed.
The total compute time is calculated by:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">total_compute_time = (records/file * total_files)/simulated_accelerators/batch_size * sleep_time</code></pre>
</div>
</div>
<div class="paragraph">
<p>The formula below calculates the AU:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">AU (percentage) = (total_compute_time/total_benchmark_running_time) * 100</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_mlperf_storage_installation"><a class="anchor" href="#_mlperf_storage_installation"></a>MLPerf Storage Installation</h4>
<div class="paragraph">
<p>First, the <code>mpich</code> for MPI package is needed. This can be done by:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo apt-get install mpich</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, clone the <a href="https://github.com/mlcommons/storage">MLCommons Storage</a> repo.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">git clone https://github.com/mlcommons/storage.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>requirements.txt</code> for <code>dlio_benchmark</code> would need to be installed. Before that, a Python virtual environment needs to be set up:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">python3 -m venv ~/myvenv
source ~/myvenv/bin/activate</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>requirements.txt</code> would then be installed afterwards.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">pip install -r dlio_benchmark/requirements.txt</code></pre>
</div>
</div>
<div class="paragraph">
<p>To launch the <code>dlio_benchmark</code>, execute the <code>benchmark.sh</code> script:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">./benchmark.sh -h</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_resnet_50"><a class="anchor" href="#_resnet_50"></a>RESNET-50</h3>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/training/blob/master/object_detection/README.md">RESNET-50 Object Detection Instructions</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/training_results_v3.0/tree/main/NVIDIA/benchmarks/resnet/implementations/mxnet">Nvidia RESNET Instructions</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The RESNET-50 neural network is a well-known image classification network that can be used with the <code>ImageNet</code> dataset. It computationally intensive and good indication of driving meaningful storage I/O. In order to keep up the training benchmark and the overall run average time, the storage system has to keep up with the read bandwidth demands of a complete training job. The training benchmark are measured at <code>Epoch 0</code>, which is the most I/O-intensive portion of the MLPerf benchmark run. On that note, the RESNET-50 test would verify if the storage system is not a bottleneck for the workload and will provide the same images/second for <code>Epoch 0</code>.</p>
</div>
<div class="sect3">
<h4 id="_resnet_50_installation"><a class="anchor" href="#_resnet_50_installation"></a>RESNET-50 Installation</h4>
<div class="paragraph">
<p>In order to run the MLPerf RESNET-50 test, <a href="https://github.com/mlcommons/ck/tree/master/cm/cmind">Collective Mind automation language</a> (CM) (also known as ML Commons CM language)  would need to be installed. It is part of the MLCommons Collective Knowledge (CK) project, and is powered by Python, JSON, YAML, and a unified CLI. More detailed information on CM  can be found here: <a href="https://github.com/mlcommons/ck/tree/master/cm#readme">Collective Minds</a>. The following installation steps assume the host machine will be running on RedHat Enterprise Linux. Furthermore, <code>python 3+</code>, <code>pip</code>, <code>git</code> , and <code>wget</code>  would need to be installed beforehand.</p>
</div>
<div class="paragraph">
<p>The following will install CM:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo dnf update
sudo dnf install python3 python-pip git wget curl
python3 -m pip install --user cmind</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once CM is installed, the next step would be to install MLCommons CK repository with automation workflows for MLPerf:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cm pull repo mlcommons@ck</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command will pre-process the dataset for a given backend. The <code>loadgen</code> would be built, and it will run the inference for all scenarios and modes.
A submission folder would be created with the test results.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cmr "run mlperf inference generate-run-cmds _submission" \
    --quiet --submitter="MLCommons" --hw_name=default --model=resnet50 --implementation=reference \
    --backend=onnxruntime --device=cpu --scenario=Offline --adr.compiler.tags=gcc  --target_qps=1 \
    --category=edge --division=open</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>target_qps</code> value would need to be updated according to the system performance for a valid submission.
The following values should be as is,</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use <code>--device=cuda</code> to run the inference on Nvidia GPU</p>
</li>
<li>
<p>Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode)</p>
</li>
<li>
<p>Use <code>--category=datacenter</code> to run datacenter scenarios</p>
</li>
<li>
<p>Use <code>--backend=tf</code> or <code>--backend=tvm-onnx</code> to use <code>tensorflow</code> and <code>tvm-onnx</code> backends, respectively</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Credit goes to <a href="https://www.linkedin.com/in/sakib-samar-23a79612a">Sakib Samar</a> for a large portion of these notes.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_nccl_tests"><a class="anchor" href="#_nccl_tests"></a>NCCL Tests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>These tests check both the performance and the correctness of <a href="https://github.com/nvidia/nccl">NCCL</a> operations (inter-GPU communication).</p>
</div>
<div class="paragraph">
<p>GitHub repositories with documentation:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/NVIDIA/nccl-tests/tree/master">NCCL Tests</a></p>
</li>
<li>
<p><a href="https://github.com/nvidia/nccl">NCCL</a></p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_installation"><a class="anchor" href="#_installation"></a>Installation</h3>
<div class="paragraph">
<p>First, you need <code>nccl</code> installed on the nodes you want to include in the benchmark.</p>
</div>
<div class="paragraph">
<p>Go ahead and clone the <code>nccl</code> repo:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">git clone https://github.com/NVIDIA/nccl.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>Change directory into the <code>nccl</code> repo and build it against your system:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cd nccl
make -j src.build</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now, install it (here we&#8217;re using Ubuntu 22.04):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Install tools to create debian packages
sudo apt install build-essential devscripts debhelper fakeroot
# Build NCCL deb package
make pkg.debian.build
ls build/pkg/deb/</code></pre>
</div>
</div>
<div class="paragraph">
<p>Your <code>.deb</code> packaages should now be in <code>build/pkg/deb</code>. You can install them using</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">apt install /root/ccarlson/nccl/build/pkg/deb/libnccl-dev_2.18.5-1+cuda12.2_amd64.deb /root/ccarlson/nccl/build/pkg/deb/libnccl2_2.18.5-1+cuda12.2_amd64.deb</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, clone your <code>nccl-tests</code> repo:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">git clone https://github.com/NVIDIA/nccl-tests.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now, change directory into it and <code>make</code> it against the installation of MPI you have on the system:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cd nccl-tests
make MPI=1 MPI_HOME=/usr/mpi/gcc/openmpi-4.1.5rc2</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should now have <code>nccl-tests</code> binaries available under <code>build/</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">root@o186i221:~/ccarlson/nccl-tests# ls build/
all_gather_perf  alltoall_perf   gather_perf     reduce_perf          scatter_perf   timer.o
all_reduce_perf  broadcast_perf  hypercube_perf  reduce_scatter_perf  sendrecv_perf  verifiable</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_usage"><a class="anchor" href="#_usage"></a>Usage</h3>
<div class="paragraph">
<p><a href="https://github.com/NVIDIA/nccl-tests/tree/master#arguments">NCCL Tests Arguments</a></p>
</div>
<div class="paragraph">
<p><code>all_reduce_perf</code> options:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">USAGE: all_reduce_perf
	[-t,--nthreads &lt;num threads&gt;]
	[-g,--ngpus &lt;gpus per thread&gt;]
	[-b,--minbytes &lt;min size in bytes&gt;]
	[-e,--maxbytes &lt;max size in bytes&gt;]
	[-i,--stepbytes &lt;increment size&gt;]
	[-f,--stepfactor &lt;increment factor&gt;]
	[-n,--iters &lt;iteration count&gt;]
	[-m,--agg_iters &lt;aggregated iteration count&gt;]
	[-w,--warmup_iters &lt;warmup iteration count&gt;]
	[-p,--parallel_init &lt;0/1&gt;]
	[-c,--check &lt;check iteration count&gt;]
	[-o,--op &lt;sum/prod/min/max/avg/mulsum/all&gt;]
	[-d,--datatype &lt;nccltype/all&gt;]
	[-r,--root &lt;root&gt;]
	[-z,--blocking &lt;0/1&gt;]
	[-y,--stream_null &lt;0/1&gt;]
	[-T,--timeout &lt;time in seconds&gt;]
	[-G,--cudagraph &lt;num graph launches&gt;]
	[-C,--report_cputime &lt;0/1&gt;]
	[-a,--average &lt;0/1/2/3&gt; report average iteration time &lt;0=RANK0/1=AVG/2=MIN/3=MAX&gt;]
	[-h,--help]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Running the NCCL tests on a single node is pretty straightforward:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">/root/ccarlson/nccl-tests/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1</code></pre>
</div>
</div>
<div class="paragraph">
<p>Running across multiple nodes should be done with MPI (<code>mpirun</code>)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">mpirun --allow-run-as-root -np 2 --mca btl_tcp_if_include eth0 --machinefile machinefile.txt --map-by "node" /root/ccarlson/nccl-tests/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1</code></pre>
</div>
</div>
<div class="paragraph">
<p>My <code>machinefile</code> for two nodes looks like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">o186i221 slots=64
o186i222 slots=64</code></pre>
</div>
</div>
<div class="paragraph">
<p>This launches two tasks via MPI (<code>-np 2</code>), one per node, each of which runs the <code>all_reduce_perf</code> binary with the specified options.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
We have to include the <code>eth0</code> interface for MPI to communicate over, otherwise it&#8217;ll try to send TCP over the IB devices which won&#8217;t work.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Example output for multiple nodes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">/build/all_reduce_perf --ngpus 8 --minbytes=128M --maxbytes=1024M --stepfactor=2 --nthreads=1 --iters=1
# nThread 1 nGpus 8 minBytes 134217728 maxBytes 1073741824 step: 2(factor) warmup iters: 5 iters: 1 agg iters: 1 validation: 1 graph: 0
#
# Using devices
#  Rank  0 Group  0 Pid 256912 on   o186i221 device  0 [0x07] NVIDIA A100-SXM4-80GB
#  Rank  1 Group  0 Pid 256912 on   o186i221 device  1 [0x0b] NVIDIA A100-SXM4-80GB
#  Rank  2 Group  0 Pid 256912 on   o186i221 device  2 [0x48] NVIDIA A100-SXM4-80GB
#  Rank  3 Group  0 Pid 256912 on   o186i221 device  3 [0x4c] NVIDIA A100-SXM4-80GB
#  Rank  4 Group  0 Pid 256912 on   o186i221 device  4 [0x88] NVIDIA A100-SXM4-80GB
#  Rank  5 Group  0 Pid 256912 on   o186i221 device  5 [0x8b] NVIDIA A100-SXM4-80GB
#  Rank  6 Group  0 Pid 256912 on   o186i221 device  6 [0xc8] NVIDIA A100-SXM4-80GB
#  Rank  7 Group  0 Pid 256912 on   o186i221 device  7 [0xcb] NVIDIA A100-SXM4-80GB
#  Rank  8 Group  0 Pid 540236 on   o186i222 device  0 [0x07] NVIDIA A100-SXM4-80GB
#  Rank  9 Group  0 Pid 540236 on   o186i222 device  1 [0x0b] NVIDIA A100-SXM4-80GB
#  Rank 10 Group  0 Pid 540236 on   o186i222 device  2 [0x48] NVIDIA A100-SXM4-80GB
#  Rank 11 Group  0 Pid 540236 on   o186i222 device  3 [0x4c] NVIDIA A100-SXM4-80GB
#  Rank 12 Group  0 Pid 540236 on   o186i222 device  4 [0x88] NVIDIA A100-SXM4-80GB
#  Rank 13 Group  0 Pid 540236 on   o186i222 device  5 [0x8b] NVIDIA A100-SXM4-80GB
#  Rank 14 Group  0 Pid 540236 on   o186i222 device  6 [0xc8] NVIDIA A100-SXM4-80GB
#  Rank 15 Group  0 Pid 540236 on   o186i222 device  7 [0xcb] NVIDIA A100-SXM4-80GB
#
#                                                              out-of-place                       in-place
#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
   134217728      33554432     float     sum      -1   5619.6   23.88   44.78      0   5662.5   23.70   44.44      0
   268435456      67108864     float     sum      -1    10796   24.86   46.62      0    10836   24.77   46.45      0
   536870912     134217728     float     sum      -1    14128   38.00   71.25      0    13863   38.73   72.61      0
  1073741824     268435456     float     sum      -1    24419   43.97   82.45      0    24255   44.27   83.00      0
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 61.4514
#</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_gpu_direct_storage_gds"><a class="anchor" href="#_gpu_direct_storage_gds"></a>GPU Direct Storage (GDS)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can verify the installation by running</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">/usr/local/cuda-12.2/gds/tools/gdscheck.py -p</code></pre>
</div>
</div>
<div class="paragraph">
<p>Example output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console"> ============
 ENVIRONMENT:
 ============
 =====================
 DRIVER CONFIGURATION:
 =====================
 NVMe               : Unsupported
 NVMeOF             : Unsupported
 SCSI               : Unsupported
 ScaleFlux CSD      : Unsupported
 NVMesh             : Unsupported
 DDN EXAScaler      : Unsupported
 IBM Spectrum Scale : Unsupported
 NFS                : Unsupported
 BeeGFS             : Unsupported
 WekaFS             : Unsupported
 Userspace RDMA     : Unsupported
 --Mellanox PeerDirect : Enabled
 --rdma library        : Not Loaded (libcufile_rdma.so)
 --rdma devices        : Not configured
 --rdma_device_status  : Up: 0 Down: 0
 =====================
 CUFILE CONFIGURATION:
 =====================
 properties.use_compat_mode : true
 properties.force_compat_mode : false
 properties.gds_rdma_write_support : true
 properties.use_poll_mode : false
 properties.poll_mode_max_size_kb : 4
 properties.max_batch_io_size : 128
 properties.max_batch_io_timeout_msecs : 5
 properties.max_direct_io_size_kb : 16384
 properties.max_device_cache_size_kb : 131072
 properties.max_device_pinned_mem_size_kb : 33554432
 properties.posix_pool_slab_size_kb : 4 1024 16384
 properties.posix_pool_slab_count : 128 64 32
 properties.rdma_peer_affinity_policy : RoundRobin
 properties.rdma_dynamic_routing : 0
 fs.generic.posix_unaligned_writes : false
 fs.lustre.posix_gds_min_kb: 0
 fs.beegfs.posix_gds_min_kb: 0
 fs.weka.rdma_write_support: false
 fs.gpfs.gds_write_support: false
 profile.nvtx : false
 profile.cufile_stats : 0
 miscellaneous.api_check_aggressive : false
 execution.max_io_threads : 4
 execution.max_io_queue_depth : 128
 execution.parallel_io : true
 execution.min_io_threshold_size_kb : 8192
 execution.max_request_parallelism : 4
 properties.force_odirect_mode : false
 properties.prefer_iouring : false
 =========
 GPU INFO:
 =========
 GPU index 0 NVIDIA A100-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 GPU index 1 NVIDIA A100-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 GPU index 2 NVIDIA A100-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 GPU index 3 NVIDIA A100-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 GPU index 4 NVIDIA A100-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 GPU index 5 NVIDIA A100-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 GPU index 6 NVIDIA A100-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 GPU index 7 NVIDIA A100-SXM4-80GB bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 ==============
 PLATFORM INFO:
 ==============
 IOMMU: disabled
 Platform verification succeeded</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_gds_benchmarking"><a class="anchor" href="#_gds_benchmarking"></a>GDS Benchmarking</h3>
<div class="paragraph">
<p><a href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#abstract">Benchmarking Guide</a></p>
</div>
<div class="paragraph">
<p>There are various parameters and settings that will factor into delivered performance. In addition to storage/filesystem-specific parameters, there are system settings and GDS-specific parameters defined in <code>/etc/cufile.json</code>. Default configuration file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
    // NOTE : Application can override custom configuration via export CUFILE_ENV_PATH_JSON=&lt;filepath&gt;
    // e.g : export CUFILE_ENV_PATH_JSON="/home/&lt;xxx&gt;/cufile.json"

            "logging": {
                            // log directory, if not enabled will create log file under current working directory
                            //"dir": "/home/&lt;xxxx&gt;",

                            // NOTICE|ERROR|WARN|INFO|DEBUG|TRACE (in decreasing order of severity)
                            "level": "ERROR"
            },

            "profile": {
                            // nvtx profiling on/off
                            "nvtx": false,
                            // cufile stats level(0-3)
                            "cufile_stats": 0
            },

            "execution" : {
                    // max number of workitems in the queue;
                    "max_io_queue_depth": 128,
                    // max number of host threads per gpu to spawn for parallel IO
                    "max_io_threads" : 4,
                    // enable support for parallel IO
                    "parallel_io" : true,
                    // minimum IO threshold before splitting the IO
                    "min_io_threshold_size_kb" : 8192,
		    // maximum parallelism for a single request
		    "max_request_parallelism" : 4
            },

            "properties": {
                            // max IO chunk size (parameter should be multiples of 64K) used by cuFileRead/Write internally per IO request
                            "max_direct_io_size_kb" : 16384,
                            // device memory size (parameter should be 4K aligned) for reserving bounce buffers for the entire GPU
                            "max_device_cache_size_kb" : 131072,
                            // limit on maximum device memory size (parameter should be 4K aligned) that can be pinned for a given process
                            "max_device_pinned_mem_size_kb" : 33554432,
                            // true or false (true will enable asynchronous io submission to nvidia-fs driver)
                            // Note : currently the overall IO will still be synchronous
                            "use_poll_mode" : false,
                            // maximum IO request size (parameter should be 4K aligned) within or equal to which library will use polling for IO completion
                            "poll_mode_max_size_kb": 4,
                            // allow compat mode, this will enable use of cuFile posix read/writes
                            "allow_compat_mode": true,
                            // enable GDS write support for RDMA based storage
                            "gds_rdma_write_support": true,
                            // GDS batch size
                            "io_batchsize": 128,
                            // enable io priority w.r.t compute streams
                            // valid options are "default", "low", "med", "high"
                            "io_priority": "default",
                            // client-side rdma addr list for user-space file-systems(e.g ["10.0.1.0", "10.0.2.0"])
                            "rdma_dev_addr_list": [ ],
                            // load balancing policy for RDMA memory registration(MR), (RoundRobin, RoundRobinMaxMin)
                            // In RoundRobin, MRs will be distributed uniformly across NICS closest to a GPU
                            // In RoundRobinMaxMin, MRs will be distributed across NICS closest to a GPU
                            // with minimal sharing of NICS acros GPUS
                            "rdma_load_balancing_policy": "RoundRobin",
			    //32-bit dc key value in hex
			    //"rdma_dc_key": "0xffeeddcc",
			    //To enable/disable different rdma OPs use the below bit map
			    //Bit 0 - If set enables Local RDMA WRITE
			    //Bit 1 - If set enables Remote RDMA WRITE
			    //Bit 2 - If set enables Remote RDMA READ
			    //Bit 3 - If set enables REMOTE RDMA Atomics
			    //Bit 4 - If set enables Relaxed ordering.
			    //"rdma_access_mask": "0x1f",

                            // In platforms where IO transfer to a GPU will cause cross RootPort PCie transfers, enabling this feature
                            // might help improve overall BW provided there exists a GPU(s) with Root Port common to that of the storage NIC(s).
                            // If this feature is enabled, please provide the ip addresses used by the mount either in file-system specific
                            // section for mount_table or in the rdma_dev_addr_list property in properties section
                            "rdma_dynamic_routing": false,
                            // The order describes the sequence in which a policy is selected for dynamic routing for cross Root Port transfers
                            // If the first policy is not applicable, it will fallback to the next and so on.
                            // policy GPU_MEM_NVLINKS: use GPU memory with NVLink to transfer data between GPUs
                            // policy GPU_MEM: use GPU memory with PCIe to transfer data between GPUs
                            // policy SYS_MEM: use system memory with PCIe to transfer data to GPU
                            // policy P2P: use P2P PCIe to transfer across between NIC and GPU
                            "rdma_dynamic_routing_order": [ "GPU_MEM_NVLINKS", "GPU_MEM", "SYS_MEM", "P2P" ]
            },

            "fs": {
                    "generic": {

                            // for unaligned writes, setting it to true will, cuFileWrite use posix write internally instead of regular GDS write
                            "posix_unaligned_writes" : false
                    },

		    "beegfs" : {
                            // IO threshold for read/write (param should be 4K aligned)) equal to or below which cuFile will use posix read/write
                            "posix_gds_min_kb" : 0

                            // To restrict the IO to selected IP list, when dynamic routing is enabled
                            // if using a single BeeGFS mount, provide the ip addresses here
                            //"rdma_dev_addr_list" : []

                            // if using multiple lustre mounts, provide ip addresses used by respective mount here
                            //"mount_table" : {
                            //                    "/beegfs/client1" : {
                            //                                    "rdma_dev_addr_list" : ["172.172.1.40", "172.172.1.42"]
                            //                    },

                            //                    "/beegfs/client2" : {
                            //                                    "rdma_dev_addr_list" : ["172.172.2.40", "172.172.2.42"]
                            //                    }
                            //}

		    },
                    "lustre": {

                            // IO threshold for read/write (param should be 4K aligned)) equal to or below which cuFile will use posix read/write
                            "posix_gds_min_kb" : 0

                            // To restrict the IO to selected IP list, when dynamic routing is enabled
                            // if using a single lustre mount, provide the ip addresses here (use : sudo lnetctl net show)
                            //"rdma_dev_addr_list" : []

                            // if using multiple lustre mounts, provide ip addresses used by respective mount here
                            //"mount_table" : {
                            //                    "/lustre/ai200_01/client" : {
                            //                                    "rdma_dev_addr_list" : ["172.172.1.40", "172.172.1.42"]
                            //                    },

                            //                    "/lustre/ai200_02/client" : {
                            //                                    "rdma_dev_addr_list" : ["172.172.2.40", "172.172.2.42"]
                            //                    }
                            //}
                    },

                    "nfs": {

                           // To restrict the IO to selected IP list, when dynamic routing is enabled
                           //"rdma_dev_addr_list" : []

                           //"mount_table" : {
                           //                     "/mnt/nfsrdma_01/" : {
                           //                                     "rdma_dev_addr_list" : []
                           //                     },

                           //                     "/mnt/nfsrdma_02/" : {
                           //                                     "rdma_dev_addr_list" : []
                           //                     }
                           //}
                    },

		    "gpfs": {
                           //allow GDS writes with GPFS
                           "gds_write_support": false

                           //"rdma_dev_addr_list" : []

                           //"mount_table" : {
                           //                     "/mnt/gpfs_01" : {
                           //                                     "rdma_dev_addr_list" : []
                           //                     },

                           //                     "/mnt/gpfs_02/" : {
                           //                                     "rdma_dev_addr_list" : []
                           //                     }
                           //}
                    },

                    "weka": {

                            // enable/disable RDMA write
                            "rdma_write_support" : false
                    }
            },

            "denylist": {
                            // specify list of vendor driver modules to deny for nvidia-fs (e.g. ["nvme" , "nvme_rdma"])
                            "drivers":  [ ],

                            // specify list of block devices to prevent IO using cuFile (e.g. [ "/dev/nvme0n1" ])
                            "devices": [ ],

                            // specify list of mount points to prevent IO using cuFile (e.g. ["/mnt/test"])
                            "mounts": [ ],

                            // specify list of file-systems to prevent IO using cuFile (e.g ["lustre", "wekafs"])
                            "filesystems": [ ]
            },

            "miscellaneous": {
                            // enable only for enforcing strict checks at API level for debugging
                            "api_check_aggressive": false
	    }
}</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="_gdsio"><a class="anchor" href="#_gdsio"></a><code>gdsio</code></h4>
<div class="paragraph">
<p>Usage help:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">gdsio version :1.11
Usage [using config file]: gdsio rw-sample.gdsio
Usage [using cmd line options]:/usr/local/cuda-12.2/gds/tools/gdsio
         -f &lt;file name&gt;
         -D &lt;directory name&gt;
         -d &lt;gpu_index (refer nvidia-smi)&gt;
         -n &lt;numa node&gt;
         -m &lt;memory type(0 - (cudaMalloc), 1 - (cuMem), 2 - (cudaMallocHost), 3 - (malloc) 4 - (mmap))&gt;
         -w &lt;number of threads for a job&gt;
         -s &lt;file size(K|M|G)&gt;
         -o &lt;start offset(K|M|G)&gt;
         -i &lt;io_size(K|M|G)&gt; &lt;min_size:max_size:step_size&gt;
         -p &lt;enable nvlinks&gt;
         -b &lt;skip bufregister&gt;
         -V &lt;verify IO&gt;
         -x &lt;xfer_type&gt; [0(GPU_DIRECT), 1(CPU_ONLY), 2(CPU_GPU), 3(CPU_ASYNC_GPU), 4(CPU_CACHED_GPU), 5(GPU_DIRECT_ASYNC), 6(GPU_BATCH), 7(GPU_BATCH_STREAM)]
         -B &lt;batch size&gt;
         -I &lt;(read) 0|(write)1| (randread) 2| (randwrite) 3&gt;
         -T &lt;duration in seconds&gt;
         -k &lt;random_seed&gt; (number e.g. 3456) to be used with random read/write&gt;
         -U &lt;use unaligned(4K) random offsets&gt;
         -R &lt;fill io buffer with random data&gt;
         -F &lt;refill io buffer with random data during each write&gt;
         -a &lt;alignment size in case of random IO&gt;
         -P &lt;rdma url&gt;
         -J &lt;per job statistics&gt;

xfer_type:
0 - Storage-&gt;GPU (GDS)
1 - Storage-&gt;CPU
2 - Storage-&gt;CPU-&gt;GPU
3 - Storage-&gt;CPU-&gt;GPU_ASYNC
4 - Storage-&gt;PAGE_CACHE-&gt;CPU-&gt;GPU
5 - Storage-&gt;GPU_ASYNC
6 - Storage-&gt;GPU_BATCH
7 - Storage-&gt;GPU_BATCH_STREAM

Note:
read test (-I 0) with verify option (-V) should be used with files written (-I 1) with -V option
read test (-I 2) with verify option (-V) should be used with files written (-I 3) with -V option, using same random seed (-k),
same number of threads(-w), offset(-o), and data size(-s)
write test (-I 1/3) with verify option (-V) will perform writes followed by read</code></pre>
</div>
</div>
<div class="paragraph">
<p>Example run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">root@o186i221:~# gdsio -I 1 -i 512K -s 16M -w 128 -d 0 -x 0 -D /mnt/cstor1/ccarlson/flash
IoType: WRITE XferType: GPUD Threads: 128 DataSetSize: 1630208/2097152(KiB) IOSize: 512(KiB) Throughput: 7.804225 GiB/sec, Avg_Latency: 6595.108546 usecs ops: 3184 total_time 0.199211 secs</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here&#8217;s a script that helps with the runs, and has comments explaning what each option does.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">#!/bin/bash

set -ex
POOL_DIR="/mnt/cstor1/ccarlson/flash"
GDSIO_BIN="/usr/local/cuda-12.2/gds/tools/gdsio"

rm -rf $POOL_DIR/*

# Set up a directory per GPU on the host
for GPU_INDEX in {0..8}; do
  mkdir -p $POOL_DIR/$(hostname)/$GPU_INDEX
done

echo "Created the following directory structure"
tree $POOL_DIR

# Optionally, mpirun gdsio instances
#mpirun --machinefile machinefile.txt --allow-run-as-root \
#	-np 1 $GDSIO_BIN -I 1 -i 512K -s 128M -p -x 0 -d 0 -w 128 -D /mnt/cstor1/ccarlson/flash/$(hostname)/0 : \
#	-np 1 $GDSIO_BIN -I 1 -i 512K -s 128M -p -x 0 -d 1 -w 128 -D /mnt/cstor1/ccarlson/flash/$(hostname)/1 : \
#	-np 1 $GDSIO_BIN -I 1 -i 512K -s 128M -p -x 0 -d 2 -w 128 -D /mnt/cstor1/ccarlson/flash/$(hostname)/2 : \
#	-np 1 $GDSIO_BIN -I 1 -i 512K -s 128M -p -x 0 -d 3 -w 128 -D /mnt/cstor1/ccarlson/flash/$(hostname)/3 : \
#	-np 1 $GDSIO_BIN -I 1 -i 512K -s 128M -p -x 0 -d 4 -w 128 -D /mnt/cstor1/ccarlson/flash/$(hostname)/4 : \
#	-np 1 $GDSIO_BIN -I 1 -i 512K -s 128M -p -x 0 -d 5 -w 128 -D /mnt/cstor1/ccarlson/flash/$(hostname)/5 : \
#	-np 1 $GDSIO_BIN -I 1 -i 512K -s 128M -p -x 0 -d 6 -w 128 -D /mnt/cstor1/ccarlson/flash/$(hostname)/6 : \
#	-np 1 $GDSIO_BIN -I 1 -i 512K -s 128M -p -x 0 -d 7 -w 128 -D /mnt/cstor1/ccarlson/flash/$(hostname)/7 \

# Run gdsio directly
gdsio -I 1 -i 1024K -s 256M -p -x 0 -d 0 -w 256 -D /mnt/cstor1/ccarlson/flash/$(hostname)/0</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <p>Copyright © 2023-2025 Caleb Carlson. All rights reserved.
</footer>
<script id="site-script" src="../../../../_/js/site.js" data-ui-root-path="../../../../_"></script>
<script async src="../../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
