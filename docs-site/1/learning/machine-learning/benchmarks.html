<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Machine Learning Benchmarks :: Caleb Carlson | Documentation</title>
    <link rel="canonical" href="https://inf0rmatiker.github.io/docs-site/1/learning/machine-learning/benchmarks.html">
    <meta name="generator" content="Antora 3.1.4">
    <link rel="stylesheet" href="../../../../_/css/site.css">
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://inf0rmatiker.github.io">Caleb Carlson | Documentation</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://inf0rmatiker.github.io/">Home</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="docs-site" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../../index.html">Documentation</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../../index.html">Overview</a>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Projects</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">VU Meter</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../projects/vu-meter/vu-meter.html">VU Meter</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">PC Exhaust</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../../projects/pc-exhaust/pc-exhaust.html">PC Exhaust</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Learning</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Kubernetes</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/crds.html">CRDs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/k8s-api-resources.html">Kubernetes Resources</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/k8s-install.html">Kubernetes Install</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../kubernetes/kubectl.html">kubectl</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Lustre</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/compiling-lustre.html">Compiling Lustre</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-client.html">Lustre Client</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-server.html">Lustre Server</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../lustre/lustre-networking.html">Lustre Networking</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Linux</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Installs</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/installs/rocky-install.html">Rocky Linux Install</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/installs/opensuse-install.html">OpenSUSE Linux Install</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/rpms.html">RPMs</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/tmux.html">Tmux</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../linux/user-management.html">User Management</a>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Networking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/linux-networking.html">Linux</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/proxies.html">Proxies</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/networking/ssh.html">SSH</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="3">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Storage</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/filesystems.html">Filesystems</a>
  </li>
  <li class="nav-item" data-depth="4">
    <a class="nav-link" href="../linux/storage/drives.html">Drives</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Docker</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../docker/registry-proxy.html">Registry Proxy</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Asciinema</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../asciinema/asciinema.html">Asciinema</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">BMC Management</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../bmc-management/bmc-management.html">BMC Management</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Version Control</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../version-control/git/git.html">Git</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../version-control/github/github.html">GitHub</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Algorithms</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../algorithms/algorithms.html">Algorithms</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Machine Learning</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="benchmarks.html">Benchmarks</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="machine-learning.html">Machine Learning</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="determinedai.html">Determined AI</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">REST APIs</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../rest-apis/api-security.html">Secure Development</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">InfiniBand</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../infiniband/infiniband.html">InfiniBand</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../infiniband/monitoring.html">Fabric Monitoring</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Documentation</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../../index.html">Documentation</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../../index.html">Documentation</a></li>
    <li>Learning</li>
    <li>Machine Learning</li>
    <li><a href="benchmarks.html">Benchmarks</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Machine Learning Benchmarks</h1>
<div class="sect1">
<h2 id="_system_tuning"><a class="anchor" href="#_system_tuning"></a>System Tuning</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The binding configuration for the GPUs needs to be correct.</p>
</div>
<div class="paragraph">
<p>The idea is to minimize latency going from CPU to GPU. Typically you have NUMA domains, you specify the closest NUMA domain for each CPU.
In our system you only have half the PCI lanes, so you have to make a mapping based on cores.</p>
</div>
<div class="paragraph">
<p>See the current mapping using the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">nvidia-smi topo -m</code></pre>
</div>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">root@o186i221:~/ccarlson/experiments# nvidia-smi topo -m
	GPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	GPU7	NIC0	NIC1	NIC2	NIC3	NIC4	CPU Affinity	NUMA Affinity
GPU0	 X 	PXB	SYS	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	SYS	48-63	3
GPU1	PXB	 X 	SYS	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	SYS	48-63	3
GPU2	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	16-31	1
GPU3	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	SYS	PXB	SYS	SYS	SYS	16-31	1
GPU4	SYS	SYS	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	PXB	SYS	SYS	112-127	7
GPU5	SYS	SYS	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	PXB	SYS	SYS	112-127	7
GPU6	SYS	SYS	SYS	SYS	SYS	SYS	 X 	PXB	SYS	SYS	SYS	SYS	PXB	80-95	5
GPU7	SYS	SYS	SYS	SYS	SYS	SYS	PXB	 X 	SYS	SYS	SYS	SYS	PXB	80-95	5
NIC0	PXB	PXB	SYS	SYS	SYS	SYS	SYS	SYS	 X 	SYS	SYS	SYS	SYS
NIC1	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	SYS	 X 	SYS	SYS	SYS
NIC2	SYS	SYS	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	 X 	SYS	SYS
NIC3	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	SYS	 X 	SYS
NIC4	SYS	SYS	SYS	SYS	SYS	SYS	PXB	PXB	SYS	SYS	SYS	SYS	 X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks

NIC Legend:

  NIC0: mlx5_0
  NIC1: mlx5_1
  NIC2: mlx5_2
  NIC3: mlx5_3
  NIC4: mlx5_4</code></pre>
</div>
</div>
<div class="paragraph">
<p>In order to prioritize CUDA devices when building with Docker, set the <code>BUILDDOCKER_BUILDKIT</code> environment variable.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">BUILDDOCKER_BUILDKIT=1 docker build</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mlperf"><a class="anchor" href="#_mlperf"></a>MLPerf</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/">Nvidia - MLPerf</a></p>
</li>
<li>
<p><a href="https://mlcommons.org/en/">MLCommons</a></p>
</li>
</ul>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>MLPerf™ benchmarks - developed by MLCommons, a consortium of AI leaders from academia, research labs, and industry - are designed to provide unbiased evaluations of training and inference performance for hardware, software, and services. They&#8217;re all conducted under prescribed conditions. To stay on the cutting edge of industry trends, MLPerf continues to evolve, holding new tests at regular intervals and adding new workloads that represent the state of the art in AI.</p>
</div>
</blockquote>
</div>
<div class="sect2">
<h3 id="_mlperf_training"><a class="anchor" href="#_mlperf_training"></a>MLPerf Training</h3>
<div class="paragraph">
<p>MLPerf Training presents three unique benchmarking challenges missing from other domains. Optimizations that improve training throughput can
increase time to solution, training is randomly determined and time to solution exhibits high variance,
and software and hardware are diverse enough to present difficulties for fairer benchmarking with the same binary, code, and even hyperparameters.
MLPerf Training tests eight different workloads across various use cases like computer vision, large language models, and recommenders.</p>
</div>
<div class="paragraph">
<p>While the repository, <a href="https://github.com/mlcommons/training">mlcommons/training</a>, is intended to serve as valid starting points for benchmark implementations,
it is not meant to be used for real performance measurements of software frameworks and hardware.
The following steps would need to be executed in order to run a benchmark.
Speeds may vary based on the reference hardware being used.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Clone the repo <a href="https://github.com/mlcommons/training">mlcommons/training</a> to the host machine.</p>
</li>
<li>
<p>Run the <code>install_cuda_docker.sh</code> script on the host machine. This will ensure that CUDA and Docker is installed. The script would also take care of setting up the correct docker dependencies.</p>
</li>
<li>
<p>While being on the host machine and outside of docker, run <code>./download_dataset.sh</code> for the dataset being used for the benchmark. The script should be run within the directory of the benchmark.</p>
</li>
<li>
<p>Once the download completes, it can be verified by running <code>./verify_dataset.sh</code> in the same manner.</p>
</li>
<li>
<p>Build and run the docker image. Each benchmark has a command to do that.</p>
</li>
<li>
<p>Once the target quality is reached, the benchmark will stop to produce timing results.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_mlperf_storage"><a class="anchor" href="#_mlperf_storage"></a>MLPerf Storage</h3>
<div class="paragraph">
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md">MLCommons - Storage</a></p>
</div>
<div class="paragraph">
<p>MLPerf Storage is a benchmark suite to characterize the performance of storage systems that support machine learning workloads.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#overview">Overview</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#installation">Installation</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#configuration">Configuration</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#workloads">Workloads</a></p>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#u-net3d">U-Net3D</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#bert">BERT</a></p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#parameters">Parameters</a></p>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#closed">CLOSED</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/storage/blob/main/README.md#open">OPEN</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>The MLPerf Storage Benchmark Suite is an AI/ML benchmarking suite that measures performance of storage for ML workloads.
The benchmark helps bridge the gap between the utilization between storage and compute resources to make sure that they are both used efficiently for ML workloads.</p>
</div>
<div class="paragraph">
<p>It measures the sustained performance of a storage system for MLPerf Training and HPC workloads on both PyTorch and Tensorflow without requiring the use of expensive accelerators. Additionally, the <code>dlio_benchmark</code> code is used to emulate I/O patterns for deep learning workloads.</p>
</div>
<div class="sect3">
<h4 id="_accelerator_utilization"><a class="anchor" href="#_accelerator_utilization"></a>Accelerator Utilization</h4>
<div class="paragraph">
<p>The Accelerator Utilization (AU) is the benchmark output metric samples per second for each workload.
In order the calculate the AU, the total compute time and the total benchmark running time is needed.
The total compute time is calculated by:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">total_compute_time = (records/file * total_files)/simulated_accelerators/batch_size * sleep_time</code></pre>
</div>
</div>
<div class="paragraph">
<p>The formula below calculates the AU:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">AU (percentage) = (total_compute_time/total_benchmark_running_time) * 100</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_mlperf_storage_installation"><a class="anchor" href="#_mlperf_storage_installation"></a>MLPerf Storage Installation</h4>
<div class="paragraph">
<p>First, the <code>mpich</code> for MPI package is needed. This can be done by:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo apt-get install mpich</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, clone the <a href="https://github.com/mlcommons/storage">MLCommons Storage</a> repo.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">git clone https://github.com/mlcommons/storage.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>requirements.txt</code> for <code>dlio_benchmark</code> would need to be installed. Before that, a Python virtual environment needs to be set up:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">python3 -m venv ~/myvenv
source ~/myvenv/bin/activate</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>requirements.txt</code> would then be installed afterwards.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">pip install -r dlio_benchmark/requirements.txt</code></pre>
</div>
</div>
<div class="paragraph">
<p>To launch the <code>dlio_benchmark</code>, execute the <code>benchmark.sh</code> script:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">./benchmark.sh -h</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_resnet_50"><a class="anchor" href="#_resnet_50"></a>RESNET-50</h3>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/mlcommons/training/blob/master/object_detection/README.md">RESNET-50 Object Detection Instructions</a></p>
</li>
<li>
<p><a href="https://github.com/mlcommons/training_results_v3.0/tree/main/NVIDIA/benchmarks/resnet/implementations/mxnet">Nvidia RESNET Instructions</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The RESNET-50 neural network is a well-known image classification network that can be used with the <code>ImageNet</code> dataset. It computationally intensive and good indication of driving meaningful storage I/O. In order to keep up the training benchmark and the overall run average time, the storage system has to keep up with the read bandwidth demands of a complete training job. The training benchmark are measured at <code>Epoch 0</code>, which is the most I/O-intensive portion of the MLPerf benchmark run. On that note, the RESNET-50 test would verify if the storage system is not a bottleneck for the workload and will provide the same images/second for <code>Epoch 0</code>.</p>
</div>
<div class="sect3">
<h4 id="_resnet_50_installation"><a class="anchor" href="#_resnet_50_installation"></a>RESNET-50 Installation</h4>
<div class="paragraph">
<p>In order to run the MLPerf RESNET-50 test, <a href="https://github.com/mlcommons/ck/tree/master/cm/cmind">Collective Mind automation language</a> (CM) (also known as ML Commons CM language)  would need to be installed. It is part of the MLCommons Collective Knowledge (CK) project, and is powered by Python, JSON, YAML, and a unified CLI. More detailed information on CM  can be found here: <a href="https://github.com/mlcommons/ck/tree/master/cm#readme">Collective Minds</a>. The following installation steps assume the host machine will be running on RedHat Enterprise Linux. Furthermore, <code>python 3+</code>, <code>pip</code>, <code>git</code> , and <code>wget</code>  would need to be installed beforehand.</p>
</div>
<div class="paragraph">
<p>The following will install CM:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">sudo dnf update
sudo dnf install python3 python-pip git wget curl
python3 -m pip install --user cmind</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once CM is installed, the next step would be to install MLCommons CK repository with automation workflows for MLPerf:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cm pull repo mlcommons@ck</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command will pre-process the dataset for a given backend. The <code>loadgen</code> would be built, and it will run the inference for all scenarios and modes.
A submission folder would be created with the test results.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cmr "run mlperf inference generate-run-cmds _submission" \
    --quiet --submitter="MLCommons" --hw_name=default --model=resnet50 --implementation=reference \
    --backend=onnxruntime --device=cpu --scenario=Offline --adr.compiler.tags=gcc  --target_qps=1 \
    --category=edge --division=open</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>target_qps</code> value would need to be updated according to the system performance for a valid submission.
The following values should be as is,</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use <code>--device=cuda</code> to run the inference on Nvidia GPU</p>
</li>
<li>
<p>Use <code>--division=closed</code> to run all scenarios for the closed division (compliance tests are skipped for <code>_find-performance</code> mode)</p>
</li>
<li>
<p>Use <code>--category=datacenter</code> to run datacenter scenarios</p>
</li>
<li>
<p>Use <code>--backend=tf</code> or <code>--backend=tvm-onnx</code> to use <code>tensorflow</code> and <code>tvm-onnx</code> backends, respectively</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Credit goes to <a href="https://www.linkedin.com/in/sakib-samar-23a79612a">Sakib Samar</a> for a large portion of these notes.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <p>Copyright © 2023 Caleb Carlson. All rights reserved.
</footer>
<script id="site-script" src="../../../../_/js/site.js" data-ui-root-path="../../../../_"></script>
<script async src="../../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
